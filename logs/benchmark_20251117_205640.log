[2025-11-17 20:56:40] INFO: ================================================================================
[2025-11-17 20:56:40] INFO: Cloud Server Strategy Benchmark
[2025-11-17 20:56:40] INFO: ================================================================================
[2025-11-17 20:56:40] INFO: Log file: logs\benchmark_20251117_205640.log
[2025-11-17 20:56:40] INFO: CSV file: results\benchmark_results_20251117_205640.csv
[2025-11-17 20:56:40] INFO: 
[2025-11-17 20:56:40] INFO: Configuration:
[2025-11-17 20:56:40] INFO:   Hardware: 12 channels, 8 TOPS
[2025-11-17 20:56:40] INFO:   Inference: past_seq_len=2048, data_format=FP16
[2025-11-17 20:56:40] INFO:   Strategies: ['h2llm', 'recursive_grid_search', 'trivial']
[2025-11-17 20:56:40] INFO:   Models: ['Qwen3-32B', 'GLM4-32B', 'Yi-34B', 'Meta-Llama-3-70B']
[2025-11-17 20:56:40] INFO:   Batch sizes: [16, 32, 64]
[2025-11-17 20:56:40] INFO: 
[2025-11-17 20:56:40] INFO: Total tests: 36
[2025-11-17 20:56:40] INFO: Starting parallel execution...
[2025-11-17 20:56:40] INFO: 
[2025-11-17 20:56:47] INFO: [1/36] ✓ Qwen3-32B batch=16 strategy=h2llm latency=228911.000000s
[2025-11-17 20:56:47] INFO: 
================================================================================
[2025-11-17 20:56:47] INFO: Detailed Latency: Qwen3-32B batch=16 strategy=h2llm
[2025-11-17 20:56:47] INFO: ================================================================================
[2025-11-17 20:56:47] INFO: 
  attention_norm
[2025-11-17 20:56:47] INFO:     Op: rmsnorm
[2025-11-17 20:56:47] INFO:     Output Shape: torch.Size([16, 1, 5120])
[2025-11-17 20:56:47] INFO:     Latency: 800.000000
[2025-11-17 20:56:47] INFO:     Cumulative: 800.000000
[2025-11-17 20:56:47] INFO: 
  wq
[2025-11-17 20:56:47] INFO:     Op: matmul
[2025-11-17 20:56:47] INFO:     Shape: 5120x5120 (batch=16)
[2025-11-17 20:56:47] INFO:     Latency: 10060.000000
[2025-11-17 20:56:47] INFO:     Cumulative: 10860.000000
[2025-11-17 20:56:47] INFO: 
  wk
[2025-11-17 20:56:47] INFO:     Op: matmul
[2025-11-17 20:56:47] INFO:     Shape: 5120x5120 (batch=16)
[2025-11-17 20:56:47] INFO:     Latency: 10060.000000
[2025-11-17 20:56:47] INFO:     Cumulative: 20920.000000
[2025-11-17 20:56:47] INFO: 
  wv
[2025-11-17 20:56:47] INFO:     Op: matmul
[2025-11-17 20:56:47] INFO:     Shape: 5120x5120 (batch=16)
[2025-11-17 20:56:47] INFO:     Latency: 10060.000000
[2025-11-17 20:56:47] INFO:     Cumulative: 30980.000000
[2025-11-17 20:56:47] INFO: 
  qk_matmul
[2025-11-17 20:56:47] INFO:     Op: batched_matmul
[2025-11-17 20:56:47] INFO:     Output Shape: torch.Size([16, 64, 1, 2048])
[2025-11-17 20:56:47] INFO:     Latency: 22528.000000
[2025-11-17 20:56:47] INFO:     Cumulative: 53508.000000
[2025-11-17 20:56:47] INFO: 
  softmax
[2025-11-17 20:56:47] INFO:     Op: softmax
[2025-11-17 20:56:47] INFO:     Output Shape: torch.Size([16, 64, 1, 2048])
[2025-11-17 20:56:47] INFO:     Latency: 24576.000000
[2025-11-17 20:56:47] INFO:     Cumulative: 78084.000000
[2025-11-17 20:56:47] INFO: 
  score_v_matmul
[2025-11-17 20:56:47] INFO:     Op: batched_matmul
[2025-11-17 20:56:47] INFO:     Output Shape: torch.Size([16, 64, 1, 80])
[2025-11-17 20:56:47] INFO:     Latency: 22528.000000
[2025-11-17 20:56:47] INFO:     Cumulative: 100612.000000
[2025-11-17 20:56:47] INFO: 
  wo
[2025-11-17 20:56:47] INFO:     Op: matmul
[2025-11-17 20:56:47] INFO:     Shape: 5120x5120 (batch=16)
[2025-11-17 20:56:47] INFO:     Latency: 10060.000000
[2025-11-17 20:56:47] INFO:     Cumulative: 110672.000000
[2025-11-17 20:56:47] INFO: 
  ffn_norm
[2025-11-17 20:56:47] INFO:     Op: rmsnorm
[2025-11-17 20:56:47] INFO:     Output Shape: torch.Size([16, 1, 5120])
[2025-11-17 20:56:47] INFO:     Latency: 800.000000
[2025-11-17 20:56:47] INFO:     Cumulative: 111472.000000
[2025-11-17 20:56:47] INFO: 
  w1
[2025-11-17 20:56:47] INFO:     Op: matmul
[2025-11-17 20:56:47] INFO:     Shape: 5120x25600 (batch=16)
[2025-11-17 20:56:47] INFO:     Latency: 37813.000000
[2025-11-17 20:56:47] INFO:     Cumulative: 149285.000000
[2025-11-17 20:56:47] INFO: 
  w3
[2025-11-17 20:56:47] INFO:     Op: matmul
[2025-11-17 20:56:47] INFO:     Shape: 5120x25600 (batch=16)
[2025-11-17 20:56:47] INFO:     Latency: 37813.000000
[2025-11-17 20:56:47] INFO:     Cumulative: 187098.000000
[2025-11-17 20:56:47] INFO: 
  silu
[2025-11-17 20:56:47] INFO:     Op: silu
[2025-11-17 20:56:47] INFO:     Output Shape: torch.Size([16, 1, 25600])
[2025-11-17 20:56:47] INFO:     Latency: 3200.000000
[2025-11-17 20:56:47] INFO:     Cumulative: 190298.000000
[2025-11-17 20:56:47] INFO: 
  mul
[2025-11-17 20:56:47] INFO:     Op: vector_mul
[2025-11-17 20:56:47] INFO:     Output Shape: torch.Size([16, 1, 25600])
[2025-11-17 20:56:47] INFO:     Latency: 800.000000
[2025-11-17 20:56:47] INFO:     Cumulative: 191098.000000
[2025-11-17 20:56:47] INFO: 
  w2
[2025-11-17 20:56:47] INFO:     Op: matmul
[2025-11-17 20:56:47] INFO:     Shape: 25600x5120 (batch=16)
[2025-11-17 20:56:47] INFO:     Latency: 37813.000000
[2025-11-17 20:56:47] INFO:     Cumulative: 228911.000000
[2025-11-17 20:58:20] INFO: [2/36] ✓ Qwen3-32B batch=16 strategy=recursive_grid_search latency=165020.000000s
[2025-11-17 20:58:20] INFO: 
================================================================================
[2025-11-17 20:58:20] INFO: Detailed Latency: Qwen3-32B batch=16 strategy=recursive_grid_search
[2025-11-17 20:58:20] INFO: ================================================================================
[2025-11-17 20:58:20] INFO: 
  fused_matrix_2369661794592
[2025-11-17 20:58:20] INFO:     Op: fusion_matrix
[2025-11-17 20:58:20] INFO:     Shape: 5120x15360 (batch=16)
[2025-11-17 20:58:20] INFO:     Latency: 19863.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 19863.000000
[2025-11-17 20:58:20] INFO: 
  qk_matmul
[2025-11-17 20:58:20] INFO:     Op: batched_matmul
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([16, 64, 1, 2048])
[2025-11-17 20:58:20] INFO:     Latency: 19613.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 39476.000000
[2025-11-17 20:58:20] INFO: 
  score_v_matmul
[2025-11-17 20:58:20] INFO:     Op: batched_matmul
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([16, 64, 1, 80])
[2025-11-17 20:58:20] INFO:     Latency: 22605.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 62081.000000
[2025-11-17 20:58:20] INFO: 
  wo
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x5120 (batch=16)
[2025-11-17 20:58:20] INFO:     Latency: 8098.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 70179.000000
[2025-11-17 20:58:20] INFO: 
  fused_matrix_2369700372960
[2025-11-17 20:58:20] INFO:     Op: fusion_matrix
[2025-11-17 20:58:20] INFO:     Shape: 5120x51200 (batch=16)
[2025-11-17 20:58:20] INFO:     Latency: 61903.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 132082.000000
[2025-11-17 20:58:20] INFO: 
  w2
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 25600x5120 (batch=16)
[2025-11-17 20:58:20] INFO:     Latency: 32938.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 165020.000000
[2025-11-17 20:58:20] INFO: [3/36] ✓ Qwen3-32B batch=16 strategy=trivial latency=275557.000000s
[2025-11-17 20:58:20] INFO: 
================================================================================
[2025-11-17 20:58:20] INFO: Detailed Latency: Qwen3-32B batch=16 strategy=trivial
[2025-11-17 20:58:20] INFO: ================================================================================
[2025-11-17 20:58:20] INFO: 
  attention_norm
[2025-11-17 20:58:20] INFO:     Op: rmsnorm
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([16, 1, 5120])
[2025-11-17 20:58:20] INFO:     Latency: 800.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 800.000000
[2025-11-17 20:58:20] INFO: 
  wq
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x5120 (batch=16)
[2025-11-17 20:58:20] INFO:     Latency: 14000.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 14800.000000
[2025-11-17 20:58:20] INFO: 
  wk
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x5120 (batch=16)
[2025-11-17 20:58:20] INFO:     Latency: 14000.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 28800.000000
[2025-11-17 20:58:20] INFO: 
  wv
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x5120 (batch=16)
[2025-11-17 20:58:20] INFO:     Latency: 14000.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 42800.000000
[2025-11-17 20:58:20] INFO: 
  qk_matmul
[2025-11-17 20:58:20] INFO:     Op: batched_matmul
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([16, 64, 1, 2048])
[2025-11-17 20:58:20] INFO:     Latency: 22528.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 65328.000000
[2025-11-17 20:58:20] INFO: 
  softmax
[2025-11-17 20:58:20] INFO:     Op: softmax
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([16, 64, 1, 2048])
[2025-11-17 20:58:20] INFO:     Latency: 24576.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 89904.000000
[2025-11-17 20:58:20] INFO: 
  score_v_matmul
[2025-11-17 20:58:20] INFO:     Op: batched_matmul
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([16, 64, 1, 80])
[2025-11-17 20:58:20] INFO:     Latency: 22528.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 112432.000000
[2025-11-17 20:58:20] INFO: 
  wo
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x5120 (batch=16)
[2025-11-17 20:58:20] INFO:     Latency: 14000.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 126432.000000
[2025-11-17 20:58:20] INFO: 
  ffn_norm
[2025-11-17 20:58:20] INFO:     Op: rmsnorm
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([16, 1, 5120])
[2025-11-17 20:58:20] INFO:     Latency: 800.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 127232.000000
[2025-11-17 20:58:20] INFO: 
  w1
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x25600 (batch=16)
[2025-11-17 20:58:20] INFO:     Latency: 38477.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 165709.000000
[2025-11-17 20:58:20] INFO: 
  w3
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x25600 (batch=16)
[2025-11-17 20:58:20] INFO:     Latency: 38477.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 204186.000000
[2025-11-17 20:58:20] INFO: 
  silu
[2025-11-17 20:58:20] INFO:     Op: silu
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([16, 1, 25600])
[2025-11-17 20:58:20] INFO:     Latency: 3200.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 207386.000000
[2025-11-17 20:58:20] INFO: 
  mul
[2025-11-17 20:58:20] INFO:     Op: vector_mul
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([16, 1, 25600])
[2025-11-17 20:58:20] INFO:     Latency: 800.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 208186.000000
[2025-11-17 20:58:20] INFO: 
  w2
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 25600x5120 (batch=16)
[2025-11-17 20:58:20] INFO:     Latency: 67371.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 275557.000000
[2025-11-17 20:58:20] INFO: [4/36] ✓ Qwen3-32B batch=32 strategy=h2llm latency=416298.000000s
[2025-11-17 20:58:20] INFO: 
================================================================================
[2025-11-17 20:58:20] INFO: Detailed Latency: Qwen3-32B batch=32 strategy=h2llm
[2025-11-17 20:58:20] INFO: ================================================================================
[2025-11-17 20:58:20] INFO: 
  attention_norm
[2025-11-17 20:58:20] INFO:     Op: rmsnorm
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([32, 1, 5120])
[2025-11-17 20:58:20] INFO:     Latency: 1600.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 1600.000000
[2025-11-17 20:58:20] INFO: 
  wq
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x5120 (batch=32)
[2025-11-17 20:58:20] INFO:     Latency: 17932.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 19532.000000
[2025-11-17 20:58:20] INFO: 
  wk
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x5120 (batch=32)
[2025-11-17 20:58:20] INFO:     Latency: 17932.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 37464.000000
[2025-11-17 20:58:20] INFO: 
  wv
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x5120 (batch=32)
[2025-11-17 20:58:20] INFO:     Latency: 17932.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 55396.000000
[2025-11-17 20:58:20] INFO: 
  qk_matmul
[2025-11-17 20:58:20] INFO:     Op: batched_matmul
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([32, 64, 1, 2048])
[2025-11-17 20:58:20] INFO:     Latency: 45056.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 100452.000000
[2025-11-17 20:58:20] INFO: 
  softmax
[2025-11-17 20:58:20] INFO:     Op: softmax
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([32, 64, 1, 2048])
[2025-11-17 20:58:20] INFO:     Latency: 49152.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 149604.000000
[2025-11-17 20:58:20] INFO: 
  score_v_matmul
[2025-11-17 20:58:20] INFO:     Op: batched_matmul
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([32, 64, 1, 80])
[2025-11-17 20:58:20] INFO:     Latency: 45056.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 194660.000000
[2025-11-17 20:58:20] INFO: 
  wo
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x5120 (batch=32)
[2025-11-17 20:58:20] INFO:     Latency: 17932.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 212592.000000
[2025-11-17 20:58:20] INFO: 
  ffn_norm
[2025-11-17 20:58:20] INFO:     Op: rmsnorm
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([32, 1, 5120])
[2025-11-17 20:58:20] INFO:     Latency: 1600.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 214192.000000
[2025-11-17 20:58:20] INFO: 
  w1
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x25600 (batch=32)
[2025-11-17 20:58:20] INFO:     Latency: 64702.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 278894.000000
[2025-11-17 20:58:20] INFO: 
  w3
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x25600 (batch=32)
[2025-11-17 20:58:20] INFO:     Latency: 64702.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 343596.000000
[2025-11-17 20:58:20] INFO: 
  silu
[2025-11-17 20:58:20] INFO:     Op: silu
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([32, 1, 25600])
[2025-11-17 20:58:20] INFO:     Latency: 6400.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 349996.000000
[2025-11-17 20:58:20] INFO: 
  mul
[2025-11-17 20:58:20] INFO:     Op: vector_mul
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([32, 1, 25600])
[2025-11-17 20:58:20] INFO:     Latency: 1600.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 351596.000000
[2025-11-17 20:58:20] INFO: 
  w2
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 25600x5120 (batch=32)
[2025-11-17 20:58:20] INFO:     Latency: 64702.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 416298.000000
[2025-11-17 20:58:20] INFO: [5/36] ✓ Qwen3-32B batch=32 strategy=recursive_grid_search latency=293733.000000s
[2025-11-17 20:58:20] INFO: 
================================================================================
[2025-11-17 20:58:20] INFO: Detailed Latency: Qwen3-32B batch=32 strategy=recursive_grid_search
[2025-11-17 20:58:20] INFO: ================================================================================
[2025-11-17 20:58:20] INFO: 
  fused_matrix_2353709151968
[2025-11-17 20:58:20] INFO:     Op: fusion_matrix
[2025-11-17 20:58:20] INFO:     Shape: 5120x15360 (batch=32)
[2025-11-17 20:58:20] INFO:     Latency: 34105.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 34105.000000
[2025-11-17 20:58:20] INFO: 
  qk_matmul
[2025-11-17 20:58:20] INFO:     Op: batched_matmul
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([32, 64, 1, 2048])
[2025-11-17 20:58:20] INFO:     Latency: 39226.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 73331.000000
[2025-11-17 20:58:20] INFO: 
  score_v_matmul
[2025-11-17 20:58:20] INFO:     Op: batched_matmul
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([32, 64, 1, 80])
[2025-11-17 20:58:20] INFO:     Latency: 45210.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 118541.000000
[2025-11-17 20:58:20] INFO: 
  wo
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x5120 (batch=32)
[2025-11-17 20:58:20] INFO:     Latency: 15321.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 133862.000000
[2025-11-17 20:58:20] INFO: 
  fused_matrix_2353706648064
[2025-11-17 20:58:20] INFO:     Op: fusion_matrix
[2025-11-17 20:58:20] INFO:     Shape: 5120x51200 (batch=32)
[2025-11-17 20:58:20] INFO:     Latency: 101935.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 235797.000000
[2025-11-17 20:58:20] INFO: 
  w2
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 25600x5120 (batch=32)
[2025-11-17 20:58:20] INFO:     Latency: 57936.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 293733.000000
[2025-11-17 20:58:20] INFO: [6/36] ✓ Qwen3-32B batch=32 strategy=trivial latency=509577.000000s
[2025-11-17 20:58:20] INFO: 
================================================================================
[2025-11-17 20:58:20] INFO: Detailed Latency: Qwen3-32B batch=32 strategy=trivial
[2025-11-17 20:58:20] INFO: ================================================================================
[2025-11-17 20:58:20] INFO: 
  attention_norm
[2025-11-17 20:58:20] INFO:     Op: rmsnorm
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([32, 1, 5120])
[2025-11-17 20:58:20] INFO:     Latency: 1600.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 1600.000000
[2025-11-17 20:58:20] INFO: 
  wq
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x5120 (batch=32)
[2025-11-17 20:58:20] INFO:     Latency: 25813.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 27413.000000
[2025-11-17 20:58:20] INFO: 
  wk
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x5120 (batch=32)
[2025-11-17 20:58:20] INFO:     Latency: 25813.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 53226.000000
[2025-11-17 20:58:20] INFO: 
  wv
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x5120 (batch=32)
[2025-11-17 20:58:20] INFO:     Latency: 25813.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 79039.000000
[2025-11-17 20:58:20] INFO: 
  qk_matmul
[2025-11-17 20:58:20] INFO:     Op: batched_matmul
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([32, 64, 1, 2048])
[2025-11-17 20:58:20] INFO:     Latency: 45056.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 124095.000000
[2025-11-17 20:58:20] INFO: 
  softmax
[2025-11-17 20:58:20] INFO:     Op: softmax
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([32, 64, 1, 2048])
[2025-11-17 20:58:20] INFO:     Latency: 49152.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 173247.000000
[2025-11-17 20:58:20] INFO: 
  score_v_matmul
[2025-11-17 20:58:20] INFO:     Op: batched_matmul
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([32, 64, 1, 80])
[2025-11-17 20:58:20] INFO:     Latency: 45056.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 218303.000000
[2025-11-17 20:58:20] INFO: 
  wo
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x5120 (batch=32)
[2025-11-17 20:58:20] INFO:     Latency: 25813.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 244116.000000
[2025-11-17 20:58:20] INFO: 
  ffn_norm
[2025-11-17 20:58:20] INFO:     Op: rmsnorm
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([32, 1, 5120])
[2025-11-17 20:58:20] INFO:     Latency: 1600.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 245716.000000
[2025-11-17 20:58:20] INFO: 
  w1
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x25600 (batch=32)
[2025-11-17 20:58:20] INFO:     Latency: 66026.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 311742.000000
[2025-11-17 20:58:20] INFO: 
  w3
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x25600 (batch=32)
[2025-11-17 20:58:20] INFO:     Latency: 66026.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 377768.000000
[2025-11-17 20:58:20] INFO: 
  silu
[2025-11-17 20:58:20] INFO:     Op: silu
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([32, 1, 25600])
[2025-11-17 20:58:20] INFO:     Latency: 6400.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 384168.000000
[2025-11-17 20:58:20] INFO: 
  mul
[2025-11-17 20:58:20] INFO:     Op: vector_mul
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([32, 1, 25600])
[2025-11-17 20:58:20] INFO:     Latency: 1600.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 385768.000000
[2025-11-17 20:58:20] INFO: 
  w2
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 25600x5120 (batch=32)
[2025-11-17 20:58:20] INFO:     Latency: 123809.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 509577.000000
[2025-11-17 20:58:20] INFO: [7/36] ✓ Qwen3-32B batch=64 strategy=h2llm latency=828486.000000s
[2025-11-17 20:58:20] INFO: 
================================================================================
[2025-11-17 20:58:20] INFO: Detailed Latency: Qwen3-32B batch=64 strategy=h2llm
[2025-11-17 20:58:20] INFO: ================================================================================
[2025-11-17 20:58:20] INFO: 
  attention_norm
[2025-11-17 20:58:20] INFO:     Op: rmsnorm
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([64, 1, 5120])
[2025-11-17 20:58:20] INFO:     Latency: 3200.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 3200.000000
[2025-11-17 20:58:20] INFO: 
  wq
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x5120 (batch=64)
[2025-11-17 20:58:20] INFO:     Latency: 35862.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 39062.000000
[2025-11-17 20:58:20] INFO: 
  wk
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x5120 (batch=64)
[2025-11-17 20:58:20] INFO:     Latency: 35862.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 74924.000000
[2025-11-17 20:58:20] INFO: 
  wv
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x5120 (batch=64)
[2025-11-17 20:58:20] INFO:     Latency: 35862.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 110786.000000
[2025-11-17 20:58:20] INFO: 
  qk_matmul
[2025-11-17 20:58:20] INFO:     Op: batched_matmul
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([64, 64, 1, 2048])
[2025-11-17 20:58:20] INFO:     Latency: 88064.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 198850.000000
[2025-11-17 20:58:20] INFO: 
  softmax
[2025-11-17 20:58:20] INFO:     Op: softmax
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([64, 64, 1, 2048])
[2025-11-17 20:58:20] INFO:     Latency: 98304.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 297154.000000
[2025-11-17 20:58:20] INFO: 
  score_v_matmul
[2025-11-17 20:58:20] INFO:     Op: batched_matmul
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([64, 64, 1, 80])
[2025-11-17 20:58:20] INFO:     Latency: 88064.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 385218.000000
[2025-11-17 20:58:20] INFO: 
  wo
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x5120 (batch=64)
[2025-11-17 20:58:20] INFO:     Latency: 35862.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 421080.000000
[2025-11-17 20:58:20] INFO: 
  ffn_norm
[2025-11-17 20:58:20] INFO:     Op: rmsnorm
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([64, 1, 5120])
[2025-11-17 20:58:20] INFO:     Latency: 3200.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 424280.000000
[2025-11-17 20:58:20] INFO: 
  w1
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x25600 (batch=64)
[2025-11-17 20:58:20] INFO:     Latency: 129402.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 553682.000000
[2025-11-17 20:58:20] INFO: 
  w3
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x25600 (batch=64)
[2025-11-17 20:58:20] INFO:     Latency: 129402.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 683084.000000
[2025-11-17 20:58:20] INFO: 
  silu
[2025-11-17 20:58:20] INFO:     Op: silu
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([64, 1, 25600])
[2025-11-17 20:58:20] INFO:     Latency: 12800.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 695884.000000
[2025-11-17 20:58:20] INFO: 
  mul
[2025-11-17 20:58:20] INFO:     Op: vector_mul
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([64, 1, 25600])
[2025-11-17 20:58:20] INFO:     Latency: 3200.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 699084.000000
[2025-11-17 20:58:20] INFO: 
  w2
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 25600x5120 (batch=64)
[2025-11-17 20:58:20] INFO:     Latency: 129402.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 828486.000000
[2025-11-17 20:58:20] INFO: [8/36] ✓ Qwen3-32B batch=64 strategy=recursive_grid_search latency=583626.000000s
[2025-11-17 20:58:20] INFO: 
================================================================================
[2025-11-17 20:58:20] INFO: Detailed Latency: Qwen3-32B batch=64 strategy=recursive_grid_search
[2025-11-17 20:58:20] INFO: ================================================================================
[2025-11-17 20:58:20] INFO: 
  fused_matrix_2941446153440
[2025-11-17 20:58:20] INFO:     Op: fusion_matrix
[2025-11-17 20:58:20] INFO:     Shape: 5120x15360 (batch=64)
[2025-11-17 20:58:20] INFO:     Latency: 68210.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 68210.000000
[2025-11-17 20:58:20] INFO: 
  qk_matmul
[2025-11-17 20:58:20] INFO:     Op: batched_matmul
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([64, 64, 1, 2048])
[2025-11-17 20:58:20] INFO:     Latency: 76669.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 144879.000000
[2025-11-17 20:58:20] INFO: 
  score_v_matmul
[2025-11-17 20:58:20] INFO:     Op: batched_matmul
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([64, 64, 1, 80])
[2025-11-17 20:58:20] INFO:     Latency: 88365.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 233244.000000
[2025-11-17 20:58:20] INFO: 
  wo
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x5120 (batch=64)
[2025-11-17 20:58:20] INFO:     Latency: 30643.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 263887.000000
[2025-11-17 20:58:20] INFO: 
  fused_matrix_2941446158288
[2025-11-17 20:58:20] INFO:     Op: fusion_matrix
[2025-11-17 20:58:20] INFO:     Shape: 5120x51200 (batch=64)
[2025-11-17 20:58:20] INFO:     Latency: 203871.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 467758.000000
[2025-11-17 20:58:20] INFO: 
  w2
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 25600x5120 (batch=64)
[2025-11-17 20:58:20] INFO:     Latency: 115868.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 583626.000000
[2025-11-17 20:58:20] INFO: [9/36] ✓ Qwen3-32B batch=64 strategy=trivial latency=1015053.000000s
[2025-11-17 20:58:20] INFO: 
================================================================================
[2025-11-17 20:58:20] INFO: Detailed Latency: Qwen3-32B batch=64 strategy=trivial
[2025-11-17 20:58:20] INFO: ================================================================================
[2025-11-17 20:58:20] INFO: 
  attention_norm
[2025-11-17 20:58:20] INFO:     Op: rmsnorm
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([64, 1, 5120])
[2025-11-17 20:58:20] INFO:     Latency: 3200.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 3200.000000
[2025-11-17 20:58:20] INFO: 
  wq
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x5120 (batch=64)
[2025-11-17 20:58:20] INFO:     Latency: 51626.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 54826.000000
[2025-11-17 20:58:20] INFO: 
  wk
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x5120 (batch=64)
[2025-11-17 20:58:20] INFO:     Latency: 51626.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 106452.000000
[2025-11-17 20:58:20] INFO: 
  wv
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x5120 (batch=64)
[2025-11-17 20:58:20] INFO:     Latency: 51626.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 158078.000000
[2025-11-17 20:58:20] INFO: 
  qk_matmul
[2025-11-17 20:58:20] INFO:     Op: batched_matmul
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([64, 64, 1, 2048])
[2025-11-17 20:58:20] INFO:     Latency: 88064.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 246142.000000
[2025-11-17 20:58:20] INFO: 
  softmax
[2025-11-17 20:58:20] INFO:     Op: softmax
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([64, 64, 1, 2048])
[2025-11-17 20:58:20] INFO:     Latency: 98304.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 344446.000000
[2025-11-17 20:58:20] INFO: 
  score_v_matmul
[2025-11-17 20:58:20] INFO:     Op: batched_matmul
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([64, 64, 1, 80])
[2025-11-17 20:58:20] INFO:     Latency: 88064.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 432510.000000
[2025-11-17 20:58:20] INFO: 
  wo
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x5120 (batch=64)
[2025-11-17 20:58:20] INFO:     Latency: 51626.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 484136.000000
[2025-11-17 20:58:20] INFO: 
  ffn_norm
[2025-11-17 20:58:20] INFO:     Op: rmsnorm
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([64, 1, 5120])
[2025-11-17 20:58:20] INFO:     Latency: 3200.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 487336.000000
[2025-11-17 20:58:20] INFO: 
  w1
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x25600 (batch=64)
[2025-11-17 20:58:20] INFO:     Latency: 132050.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 619386.000000
[2025-11-17 20:58:20] INFO: 
  w3
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 5120x25600 (batch=64)
[2025-11-17 20:58:20] INFO:     Latency: 132050.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 751436.000000
[2025-11-17 20:58:20] INFO: 
  silu
[2025-11-17 20:58:20] INFO:     Op: silu
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([64, 1, 25600])
[2025-11-17 20:58:20] INFO:     Latency: 12800.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 764236.000000
[2025-11-17 20:58:20] INFO: 
  mul
[2025-11-17 20:58:20] INFO:     Op: vector_mul
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([64, 1, 25600])
[2025-11-17 20:58:20] INFO:     Latency: 3200.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 767436.000000
[2025-11-17 20:58:20] INFO: 
  w2
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 25600x5120 (batch=64)
[2025-11-17 20:58:20] INFO:     Latency: 247617.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 1015053.000000
[2025-11-17 20:58:20] INFO: [10/36] ✓ GLM4-32B batch=16 strategy=h2llm latency=232713.000000s
[2025-11-17 20:58:20] INFO: 
================================================================================
[2025-11-17 20:58:20] INFO: Detailed Latency: GLM4-32B batch=16 strategy=h2llm
[2025-11-17 20:58:20] INFO: ================================================================================
[2025-11-17 20:58:20] INFO: 
  attention_norm
[2025-11-17 20:58:20] INFO:     Op: rmsnorm
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([16, 1, 6144])
[2025-11-17 20:58:20] INFO:     Latency: 960.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 960.000000
[2025-11-17 20:58:20] INFO: 
  wq
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 6144x6144 (batch=16)
[2025-11-17 20:58:20] INFO:     Latency: 13380.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 14340.000000
[2025-11-17 20:58:20] INFO: 
  wk
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 6144x6144 (batch=16)
[2025-11-17 20:58:20] INFO:     Latency: 13380.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 27720.000000
[2025-11-17 20:58:20] INFO: 
  wv
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 6144x6144 (batch=16)
[2025-11-17 20:58:20] INFO:     Latency: 13380.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 41100.000000
[2025-11-17 20:58:20] INFO: 
  qk_matmul
[2025-11-17 20:58:20] INFO:     Op: batched_matmul
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([16, 48, 1, 2048])
[2025-11-17 20:58:20] INFO:     Latency: 17430.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 58530.000000
[2025-11-17 20:58:20] INFO: 
  softmax
[2025-11-17 20:58:20] INFO:     Op: softmax
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([16, 48, 1, 2048])
[2025-11-17 20:58:20] INFO:     Latency: 18432.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 76962.000000
[2025-11-17 20:58:20] INFO: 
  score_v_matmul
[2025-11-17 20:58:20] INFO:     Op: batched_matmul
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([16, 48, 1, 128])
[2025-11-17 20:58:20] INFO:     Latency: 17430.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 94392.000000
[2025-11-17 20:58:20] INFO: 
  wo
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 6144x6144 (batch=16)
[2025-11-17 20:58:20] INFO:     Latency: 13380.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 107772.000000
[2025-11-17 20:58:20] INFO: 
  ffn_norm
[2025-11-17 20:58:20] INFO:     Op: rmsnorm
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([16, 1, 6144])
[2025-11-17 20:58:20] INFO:     Latency: 960.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 108732.000000
[2025-11-17 20:58:20] INFO: 
  w1
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 6144x23040 (batch=16)
[2025-11-17 20:58:20] INFO:     Latency: 40127.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 148859.000000
[2025-11-17 20:58:20] INFO: 
  w3
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 6144x23040 (batch=16)
[2025-11-17 20:58:20] INFO:     Latency: 40127.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 188986.000000
[2025-11-17 20:58:20] INFO: 
  silu
[2025-11-17 20:58:20] INFO:     Op: silu
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([16, 1, 23040])
[2025-11-17 20:58:20] INFO:     Latency: 2880.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 191866.000000
[2025-11-17 20:58:20] INFO: 
  mul
[2025-11-17 20:58:20] INFO:     Op: vector_mul
[2025-11-17 20:58:20] INFO:     Output Shape: torch.Size([16, 1, 23040])
[2025-11-17 20:58:20] INFO:     Latency: 720.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 192586.000000
[2025-11-17 20:58:20] INFO: 
  w2
[2025-11-17 20:58:20] INFO:     Op: matmul
[2025-11-17 20:58:20] INFO:     Shape: 23040x6144 (batch=16)
[2025-11-17 20:58:20] INFO:     Latency: 40127.000000
[2025-11-17 20:58:20] INFO:     Cumulative: 232713.000000
[2025-11-17 20:58:56] INFO: [11/36] ✓ GLM4-32B batch=16 strategy=recursive_grid_search latency=172444.000000s
[2025-11-17 20:58:56] INFO: 
================================================================================
[2025-11-17 20:58:56] INFO: Detailed Latency: GLM4-32B batch=16 strategy=recursive_grid_search
[2025-11-17 20:58:56] INFO: ================================================================================
[2025-11-17 20:58:56] INFO: 
  fused_matrix_2183310074000
[2025-11-17 20:58:56] INFO:     Op: fusion_matrix
[2025-11-17 20:58:56] INFO:     Shape: 6144x18432 (batch=16)
[2025-11-17 20:58:56] INFO:     Latency: 27522.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 27522.000000
[2025-11-17 20:58:56] INFO: 
  qk_matmul
[2025-11-17 20:58:56] INFO:     Op: batched_matmul
[2025-11-17 20:58:56] INFO:     Output Shape: torch.Size([16, 48, 1, 2048])
[2025-11-17 20:58:56] INFO:     Latency: 16281.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 43803.000000
[2025-11-17 20:58:56] INFO: 
  score_v_matmul
[2025-11-17 20:58:56] INFO:     Op: batched_matmul
[2025-11-17 20:58:56] INFO:     Output Shape: torch.Size([16, 48, 1, 128])
[2025-11-17 20:58:56] INFO:     Latency: 17460.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 61263.000000
[2025-11-17 20:58:56] INFO: 
  wo
[2025-11-17 20:58:56] INFO:     Op: matmul
[2025-11-17 20:58:56] INFO:     Shape: 6144x6144 (batch=16)
[2025-11-17 20:58:56] INFO:     Latency: 10406.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 71669.000000
[2025-11-17 20:58:56] INFO: 
  fused_matrix_2183310073712
[2025-11-17 20:58:56] INFO:     Op: fusion_matrix
[2025-11-17 20:58:56] INFO:     Shape: 6144x46080 (batch=16)
[2025-11-17 20:58:56] INFO:     Latency: 66020.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 137689.000000
[2025-11-17 20:58:56] INFO: 
  w2
[2025-11-17 20:58:56] INFO:     Op: matmul
[2025-11-17 20:58:56] INFO:     Shape: 23040x6144 (batch=16)
[2025-11-17 20:58:56] INFO:     Latency: 34755.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 172444.000000
[2025-11-17 20:58:56] INFO: [12/36] ✓ GLM4-32B batch=16 strategy=trivial latency=280761.000000s
[2025-11-17 20:58:56] INFO: 
================================================================================
[2025-11-17 20:58:56] INFO: Detailed Latency: GLM4-32B batch=16 strategy=trivial
[2025-11-17 20:58:56] INFO: ================================================================================
[2025-11-17 20:58:56] INFO: 
  attention_norm
[2025-11-17 20:58:56] INFO:     Op: rmsnorm
[2025-11-17 20:58:56] INFO:     Output Shape: torch.Size([16, 1, 6144])
[2025-11-17 20:58:56] INFO:     Latency: 960.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 960.000000
[2025-11-17 20:58:56] INFO: 
  wq
[2025-11-17 20:58:56] INFO:     Op: matmul
[2025-11-17 20:58:56] INFO:     Shape: 6144x6144 (batch=16)
[2025-11-17 20:58:56] INFO:     Latency: 18106.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 19066.000000
[2025-11-17 20:58:56] INFO: 
  wk
[2025-11-17 20:58:56] INFO:     Op: matmul
[2025-11-17 20:58:56] INFO:     Shape: 6144x6144 (batch=16)
[2025-11-17 20:58:56] INFO:     Latency: 18106.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 37172.000000
[2025-11-17 20:58:56] INFO: 
  wv
[2025-11-17 20:58:56] INFO:     Op: matmul
[2025-11-17 20:58:56] INFO:     Shape: 6144x6144 (batch=16)
[2025-11-17 20:58:56] INFO:     Latency: 18106.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 55278.000000
[2025-11-17 20:58:56] INFO: 
  qk_matmul
[2025-11-17 20:58:56] INFO:     Op: batched_matmul
[2025-11-17 20:58:56] INFO:     Output Shape: torch.Size([16, 48, 1, 2048])
[2025-11-17 20:58:56] INFO:     Latency: 17430.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 72708.000000
[2025-11-17 20:58:56] INFO: 
  softmax
[2025-11-17 20:58:56] INFO:     Op: softmax
[2025-11-17 20:58:56] INFO:     Output Shape: torch.Size([16, 48, 1, 2048])
[2025-11-17 20:58:56] INFO:     Latency: 18432.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 91140.000000
[2025-11-17 20:58:56] INFO: 
  score_v_matmul
[2025-11-17 20:58:56] INFO:     Op: batched_matmul
[2025-11-17 20:58:56] INFO:     Output Shape: torch.Size([16, 48, 1, 128])
[2025-11-17 20:58:56] INFO:     Latency: 17430.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 108570.000000
[2025-11-17 20:58:56] INFO: 
  wo
[2025-11-17 20:58:56] INFO:     Op: matmul
[2025-11-17 20:58:56] INFO:     Shape: 6144x6144 (batch=16)
[2025-11-17 20:58:56] INFO:     Latency: 18106.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 126676.000000
[2025-11-17 20:58:56] INFO: 
  ffn_norm
[2025-11-17 20:58:56] INFO:     Op: rmsnorm
[2025-11-17 20:58:56] INFO:     Output Shape: torch.Size([16, 1, 6144])
[2025-11-17 20:58:56] INFO:     Latency: 960.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 127636.000000
[2025-11-17 20:58:56] INFO: 
  w1
[2025-11-17 20:58:56] INFO:     Op: matmul
[2025-11-17 20:58:56] INFO:     Shape: 6144x23040 (batch=16)
[2025-11-17 20:58:56] INFO:     Latency: 41899.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 169535.000000
[2025-11-17 20:58:56] INFO: 
  w3
[2025-11-17 20:58:56] INFO:     Op: matmul
[2025-11-17 20:58:56] INFO:     Shape: 6144x23040 (batch=16)
[2025-11-17 20:58:56] INFO:     Latency: 41899.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 211434.000000
[2025-11-17 20:58:56] INFO: 
  silu
[2025-11-17 20:58:56] INFO:     Op: silu
[2025-11-17 20:58:56] INFO:     Output Shape: torch.Size([16, 1, 23040])
[2025-11-17 20:58:56] INFO:     Latency: 2880.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 214314.000000
[2025-11-17 20:58:56] INFO: 
  mul
[2025-11-17 20:58:56] INFO:     Op: vector_mul
[2025-11-17 20:58:56] INFO:     Output Shape: torch.Size([16, 1, 23040])
[2025-11-17 20:58:56] INFO:     Latency: 720.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 215034.000000
[2025-11-17 20:58:56] INFO: 
  w2
[2025-11-17 20:58:56] INFO:     Op: matmul
[2025-11-17 20:58:56] INFO:     Shape: 23040x6144 (batch=16)
[2025-11-17 20:58:56] INFO:     Latency: 65727.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 280761.000000
[2025-11-17 20:58:56] INFO: [13/36] ✓ GLM4-32B batch=32 strategy=h2llm latency=417437.000000s
[2025-11-17 20:58:56] INFO: 
================================================================================
[2025-11-17 20:58:56] INFO: Detailed Latency: GLM4-32B batch=32 strategy=h2llm
[2025-11-17 20:58:56] INFO: ================================================================================
[2025-11-17 20:58:56] INFO: 
  attention_norm
[2025-11-17 20:58:56] INFO:     Op: rmsnorm
[2025-11-17 20:58:56] INFO:     Output Shape: torch.Size([32, 1, 6144])
[2025-11-17 20:58:56] INFO:     Latency: 1920.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 1920.000000
[2025-11-17 20:58:56] INFO: 
  wq
[2025-11-17 20:58:56] INFO:     Op: matmul
[2025-11-17 20:58:56] INFO:     Shape: 6144x6144 (batch=32)
[2025-11-17 20:58:56] INFO:     Latency: 23612.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 25532.000000
[2025-11-17 20:58:56] INFO: 
  wk
[2025-11-17 20:58:56] INFO:     Op: matmul
[2025-11-17 20:58:56] INFO:     Shape: 6144x6144 (batch=32)
[2025-11-17 20:58:56] INFO:     Latency: 23612.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 49144.000000
[2025-11-17 20:58:56] INFO: 
  wv
[2025-11-17 20:58:56] INFO:     Op: matmul
[2025-11-17 20:58:56] INFO:     Shape: 6144x6144 (batch=32)
[2025-11-17 20:58:56] INFO:     Latency: 23612.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 72756.000000
[2025-11-17 20:58:56] INFO: 
  qk_matmul
[2025-11-17 20:58:56] INFO:     Op: batched_matmul
[2025-11-17 20:58:56] INFO:     Output Shape: torch.Size([32, 48, 1, 2048])
[2025-11-17 20:58:56] INFO:     Latency: 34860.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 107616.000000
[2025-11-17 20:58:56] INFO: 
  softmax
[2025-11-17 20:58:56] INFO:     Op: softmax
[2025-11-17 20:58:56] INFO:     Output Shape: torch.Size([32, 48, 1, 2048])
[2025-11-17 20:58:56] INFO:     Latency: 36864.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 144480.000000
[2025-11-17 20:58:56] INFO: 
  score_v_matmul
[2025-11-17 20:58:56] INFO:     Op: batched_matmul
[2025-11-17 20:58:56] INFO:     Output Shape: torch.Size([32, 48, 1, 128])
[2025-11-17 20:58:56] INFO:     Latency: 34860.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 179340.000000
[2025-11-17 20:58:56] INFO: 
  wo
[2025-11-17 20:58:56] INFO:     Op: matmul
[2025-11-17 20:58:56] INFO:     Shape: 6144x6144 (batch=32)
[2025-11-17 20:58:56] INFO:     Latency: 23612.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 202952.000000
[2025-11-17 20:58:56] INFO: 
  ffn_norm
[2025-11-17 20:58:56] INFO:     Op: rmsnorm
[2025-11-17 20:58:56] INFO:     Output Shape: torch.Size([32, 1, 6144])
[2025-11-17 20:58:56] INFO:     Latency: 1920.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 204872.000000
[2025-11-17 20:58:56] INFO: 
  w1
[2025-11-17 20:58:56] INFO:     Op: matmul
[2025-11-17 20:58:56] INFO:     Shape: 6144x23040 (batch=32)
[2025-11-17 20:58:56] INFO:     Latency: 68455.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 273327.000000
[2025-11-17 20:58:56] INFO: 
  w3
[2025-11-17 20:58:56] INFO:     Op: matmul
[2025-11-17 20:58:56] INFO:     Shape: 6144x23040 (batch=32)
[2025-11-17 20:58:56] INFO:     Latency: 68455.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 341782.000000
[2025-11-17 20:58:56] INFO: 
  silu
[2025-11-17 20:58:56] INFO:     Op: silu
[2025-11-17 20:58:56] INFO:     Output Shape: torch.Size([32, 1, 23040])
[2025-11-17 20:58:56] INFO:     Latency: 5760.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 347542.000000
[2025-11-17 20:58:56] INFO: 
  mul
[2025-11-17 20:58:56] INFO:     Op: vector_mul
[2025-11-17 20:58:56] INFO:     Output Shape: torch.Size([32, 1, 23040])
[2025-11-17 20:58:56] INFO:     Latency: 1440.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 348982.000000
[2025-11-17 20:58:56] INFO: 
  w2
[2025-11-17 20:58:56] INFO:     Op: matmul
[2025-11-17 20:58:56] INFO:     Shape: 23040x6144 (batch=32)
[2025-11-17 20:58:56] INFO:     Latency: 68455.000000
[2025-11-17 20:58:56] INFO:     Cumulative: 417437.000000
[2025-11-17 21:31:33] INFO: [14/36] ✓ GLM4-32B batch=32 strategy=recursive_grid_search latency=301279.000000s
[2025-11-17 21:31:33] INFO: 
================================================================================
[2025-11-17 21:31:33] INFO: Detailed Latency: GLM4-32B batch=32 strategy=recursive_grid_search
[2025-11-17 21:31:33] INFO: ================================================================================
[2025-11-17 21:31:33] INFO: 
  fused_matrix_2948390291600
[2025-11-17 21:31:33] INFO:     Op: fusion_matrix
[2025-11-17 21:31:33] INFO:     Shape: 6144x18432 (batch=32)
[2025-11-17 21:31:33] INFO:     Latency: 46338.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 46338.000000
[2025-11-17 21:31:33] INFO: 
  qk_matmul
[2025-11-17 21:31:33] INFO:     Op: batched_matmul
[2025-11-17 21:31:33] INFO:     Output Shape: torch.Size([32, 48, 1, 2048])
[2025-11-17 21:31:33] INFO:     Latency: 32562.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 78900.000000
[2025-11-17 21:31:33] INFO: 
  score_v_matmul
[2025-11-17 21:31:33] INFO:     Op: batched_matmul
[2025-11-17 21:31:33] INFO:     Output Shape: torch.Size([32, 48, 1, 128])
[2025-11-17 21:31:33] INFO:     Latency: 34920.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 113820.000000
[2025-11-17 21:31:33] INFO: 
  wo
[2025-11-17 21:31:33] INFO:     Op: matmul
[2025-11-17 21:31:33] INFO:     Shape: 6144x6144 (batch=32)
[2025-11-17 21:31:33] INFO:     Latency: 18925.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 132745.000000
[2025-11-17 21:31:33] INFO: 
  fused_matrix_2948466403008
[2025-11-17 21:31:33] INFO:     Op: fusion_matrix
[2025-11-17 21:31:33] INFO:     Shape: 6144x46080 (batch=32)
[2025-11-17 21:31:33] INFO:     Latency: 110064.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 242809.000000
[2025-11-17 21:31:33] INFO: 
  w2
[2025-11-17 21:31:33] INFO:     Op: matmul
[2025-11-17 21:31:33] INFO:     Shape: 23040x6144 (batch=32)
[2025-11-17 21:31:33] INFO:     Latency: 58470.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 301279.000000
[2025-11-17 21:31:33] INFO: [15/36] ✓ GLM4-32B batch=32 strategy=trivial latency=513533.000000s
[2025-11-17 21:31:33] INFO: 
================================================================================
[2025-11-17 21:31:33] INFO: Detailed Latency: GLM4-32B batch=32 strategy=trivial
[2025-11-17 21:31:33] INFO: ================================================================================
[2025-11-17 21:31:33] INFO: 
  attention_norm
[2025-11-17 21:31:33] INFO:     Op: rmsnorm
[2025-11-17 21:31:33] INFO:     Output Shape: torch.Size([32, 1, 6144])
[2025-11-17 21:31:33] INFO:     Latency: 1920.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 1920.000000
[2025-11-17 21:31:33] INFO: 
  wq
[2025-11-17 21:31:33] INFO:     Op: matmul
[2025-11-17 21:31:33] INFO:     Shape: 6144x6144 (batch=32)
[2025-11-17 21:31:33] INFO:     Latency: 33064.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 34984.000000
[2025-11-17 21:31:33] INFO: 
  wk
[2025-11-17 21:31:33] INFO:     Op: matmul
[2025-11-17 21:31:33] INFO:     Shape: 6144x6144 (batch=32)
[2025-11-17 21:31:33] INFO:     Latency: 33064.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 68048.000000
[2025-11-17 21:31:33] INFO: 
  wv
[2025-11-17 21:31:33] INFO:     Op: matmul
[2025-11-17 21:31:33] INFO:     Shape: 6144x6144 (batch=32)
[2025-11-17 21:31:33] INFO:     Latency: 33064.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 101112.000000
[2025-11-17 21:31:33] INFO: 
  qk_matmul
[2025-11-17 21:31:33] INFO:     Op: batched_matmul
[2025-11-17 21:31:33] INFO:     Output Shape: torch.Size([32, 48, 1, 2048])
[2025-11-17 21:31:33] INFO:     Latency: 34860.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 135972.000000
[2025-11-17 21:31:33] INFO: 
  softmax
[2025-11-17 21:31:33] INFO:     Op: softmax
[2025-11-17 21:31:33] INFO:     Output Shape: torch.Size([32, 48, 1, 2048])
[2025-11-17 21:31:33] INFO:     Latency: 36864.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 172836.000000
[2025-11-17 21:31:33] INFO: 
  score_v_matmul
[2025-11-17 21:31:33] INFO:     Op: batched_matmul
[2025-11-17 21:31:33] INFO:     Output Shape: torch.Size([32, 48, 1, 128])
[2025-11-17 21:31:33] INFO:     Latency: 34860.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 207696.000000
[2025-11-17 21:31:33] INFO: 
  wo
[2025-11-17 21:31:33] INFO:     Op: matmul
[2025-11-17 21:31:33] INFO:     Shape: 6144x6144 (batch=32)
[2025-11-17 21:31:33] INFO:     Latency: 33064.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 240760.000000
[2025-11-17 21:31:33] INFO: 
  ffn_norm
[2025-11-17 21:31:33] INFO:     Op: rmsnorm
[2025-11-17 21:31:33] INFO:     Output Shape: torch.Size([32, 1, 6144])
[2025-11-17 21:31:33] INFO:     Latency: 1920.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 242680.000000
[2025-11-17 21:31:33] INFO: 
  w1
[2025-11-17 21:31:33] INFO:     Op: matmul
[2025-11-17 21:31:33] INFO:     Shape: 6144x23040 (batch=32)
[2025-11-17 21:31:33] INFO:     Latency: 71999.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 314679.000000
[2025-11-17 21:31:33] INFO: 
  w3
[2025-11-17 21:31:33] INFO:     Op: matmul
[2025-11-17 21:31:33] INFO:     Shape: 6144x23040 (batch=32)
[2025-11-17 21:31:33] INFO:     Latency: 71999.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 386678.000000
[2025-11-17 21:31:33] INFO: 
  silu
[2025-11-17 21:31:33] INFO:     Op: silu
[2025-11-17 21:31:33] INFO:     Output Shape: torch.Size([32, 1, 23040])
[2025-11-17 21:31:33] INFO:     Latency: 5760.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 392438.000000
[2025-11-17 21:31:33] INFO: 
  mul
[2025-11-17 21:31:33] INFO:     Op: vector_mul
[2025-11-17 21:31:33] INFO:     Output Shape: torch.Size([32, 1, 23040])
[2025-11-17 21:31:33] INFO:     Latency: 1440.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 393878.000000
[2025-11-17 21:31:33] INFO: 
  w2
[2025-11-17 21:31:33] INFO:     Op: matmul
[2025-11-17 21:31:33] INFO:     Shape: 23040x6144 (batch=32)
[2025-11-17 21:31:33] INFO:     Latency: 119655.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 513533.000000
[2025-11-17 21:31:33] INFO: [16/36] ✓ GLM4-32B batch=64 strategy=h2llm latency=823244.000000s
[2025-11-17 21:31:33] INFO: 
================================================================================
[2025-11-17 21:31:33] INFO: Detailed Latency: GLM4-32B batch=64 strategy=h2llm
[2025-11-17 21:31:33] INFO: ================================================================================
[2025-11-17 21:31:33] INFO: 
  attention_norm
[2025-11-17 21:31:33] INFO:     Op: rmsnorm
[2025-11-17 21:31:33] INFO:     Output Shape: torch.Size([64, 1, 6144])
[2025-11-17 21:31:33] INFO:     Latency: 3840.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 3840.000000
[2025-11-17 21:31:33] INFO: 
  wq
[2025-11-17 21:31:33] INFO:     Op: matmul
[2025-11-17 21:31:33] INFO:     Shape: 6144x6144 (batch=64)
[2025-11-17 21:31:33] INFO:     Latency: 47223.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 51063.000000
[2025-11-17 21:31:33] INFO: 
  wk
[2025-11-17 21:31:33] INFO:     Op: matmul
[2025-11-17 21:31:33] INFO:     Shape: 6144x6144 (batch=64)
[2025-11-17 21:31:33] INFO:     Latency: 47223.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 98286.000000
[2025-11-17 21:31:33] INFO: 
  wv
[2025-11-17 21:31:33] INFO:     Op: matmul
[2025-11-17 21:31:33] INFO:     Shape: 6144x6144 (batch=64)
[2025-11-17 21:31:33] INFO:     Latency: 47223.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 145509.000000
[2025-11-17 21:31:33] INFO: 
  qk_matmul
[2025-11-17 21:31:33] INFO:     Op: batched_matmul
[2025-11-17 21:31:33] INFO:     Output Shape: torch.Size([64, 48, 1, 2048])
[2025-11-17 21:31:33] INFO:     Latency: 63910.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 209419.000000
[2025-11-17 21:31:33] INFO: 
  softmax
[2025-11-17 21:31:33] INFO:     Op: softmax
[2025-11-17 21:31:33] INFO:     Output Shape: torch.Size([64, 48, 1, 2048])
[2025-11-17 21:31:33] INFO:     Latency: 73728.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 283147.000000
[2025-11-17 21:31:33] INFO: 
  score_v_matmul
[2025-11-17 21:31:33] INFO:     Op: batched_matmul
[2025-11-17 21:31:33] INFO:     Output Shape: torch.Size([64, 48, 1, 128])
[2025-11-17 21:31:33] INFO:     Latency: 63910.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 347057.000000
[2025-11-17 21:31:33] INFO: 
  wo
[2025-11-17 21:31:33] INFO:     Op: matmul
[2025-11-17 21:31:33] INFO:     Shape: 6144x6144 (batch=64)
[2025-11-17 21:31:33] INFO:     Latency: 47223.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 394280.000000
[2025-11-17 21:31:33] INFO: 
  ffn_norm
[2025-11-17 21:31:33] INFO:     Op: rmsnorm
[2025-11-17 21:31:33] INFO:     Output Shape: torch.Size([64, 1, 6144])
[2025-11-17 21:31:33] INFO:     Latency: 3840.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 398120.000000
[2025-11-17 21:31:33] INFO: 
  w1
[2025-11-17 21:31:33] INFO:     Op: matmul
[2025-11-17 21:31:33] INFO:     Shape: 6144x23040 (batch=64)
[2025-11-17 21:31:33] INFO:     Latency: 136908.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 535028.000000
[2025-11-17 21:31:33] INFO: 
  w3
[2025-11-17 21:31:33] INFO:     Op: matmul
[2025-11-17 21:31:33] INFO:     Shape: 6144x23040 (batch=64)
[2025-11-17 21:31:33] INFO:     Latency: 136908.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 671936.000000
[2025-11-17 21:31:33] INFO: 
  silu
[2025-11-17 21:31:33] INFO:     Op: silu
[2025-11-17 21:31:33] INFO:     Output Shape: torch.Size([64, 1, 23040])
[2025-11-17 21:31:33] INFO:     Latency: 11520.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 683456.000000
[2025-11-17 21:31:33] INFO: 
  mul
[2025-11-17 21:31:33] INFO:     Op: vector_mul
[2025-11-17 21:31:33] INFO:     Output Shape: torch.Size([64, 1, 23040])
[2025-11-17 21:31:33] INFO:     Latency: 2880.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 686336.000000
[2025-11-17 21:31:33] INFO: 
  w2
[2025-11-17 21:31:33] INFO:     Op: matmul
[2025-11-17 21:31:33] INFO:     Shape: 23040x6144 (batch=64)
[2025-11-17 21:31:33] INFO:     Latency: 136908.000000
[2025-11-17 21:31:33] INFO:     Cumulative: 823244.000000
[2025-11-17 21:31:46] INFO: [17/36] ✓ GLM4-32B batch=64 strategy=recursive_grid_search latency=591305.000000s
[2025-11-17 21:31:46] INFO: 
================================================================================
[2025-11-17 21:31:46] INFO: Detailed Latency: GLM4-32B batch=64 strategy=recursive_grid_search
[2025-11-17 21:31:46] INFO: ================================================================================
[2025-11-17 21:31:46] INFO: 
  fused_matrix_2354327035904
[2025-11-17 21:31:46] INFO:     Op: fusion_matrix
[2025-11-17 21:31:46] INFO:     Shape: 6144x18432 (batch=64)
[2025-11-17 21:31:46] INFO:     Latency: 92674.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 92674.000000
[2025-11-17 21:31:46] INFO: 
  qk_matmul
[2025-11-17 21:31:46] INFO:     Op: batched_matmul
[2025-11-17 21:31:46] INFO:     Output Shape: torch.Size([64, 48, 1, 2048])
[2025-11-17 21:31:46] INFO:     Latency: 59697.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 152371.000000
[2025-11-17 21:31:46] INFO: 
  score_v_matmul
[2025-11-17 21:31:46] INFO:     Op: batched_matmul
[2025-11-17 21:31:46] INFO:     Output Shape: torch.Size([64, 48, 1, 128])
[2025-11-17 21:31:46] INFO:     Latency: 64020.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 216391.000000
[2025-11-17 21:31:46] INFO: 
  wo
[2025-11-17 21:31:46] INFO:     Op: matmul
[2025-11-17 21:31:46] INFO:     Shape: 6144x6144 (batch=64)
[2025-11-17 21:31:46] INFO:     Latency: 37849.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 254240.000000
[2025-11-17 21:31:46] INFO: 
  fused_matrix_2354326854192
[2025-11-17 21:31:46] INFO:     Op: fusion_matrix
[2025-11-17 21:31:46] INFO:     Shape: 6144x46080 (batch=64)
[2025-11-17 21:31:46] INFO:     Latency: 220127.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 474367.000000
[2025-11-17 21:31:46] INFO: 
  w2
[2025-11-17 21:31:46] INFO:     Op: matmul
[2025-11-17 21:31:46] INFO:     Shape: 23040x6144 (batch=64)
[2025-11-17 21:31:46] INFO:     Latency: 116938.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 591305.000000
[2025-11-17 21:31:46] INFO: [18/36] ✓ GLM4-32B batch=64 strategy=trivial latency=1015440.000000s
[2025-11-17 21:31:46] INFO: 
================================================================================
[2025-11-17 21:31:46] INFO: Detailed Latency: GLM4-32B batch=64 strategy=trivial
[2025-11-17 21:31:46] INFO: ================================================================================
[2025-11-17 21:31:46] INFO: 
  attention_norm
[2025-11-17 21:31:46] INFO:     Op: rmsnorm
[2025-11-17 21:31:46] INFO:     Output Shape: torch.Size([64, 1, 6144])
[2025-11-17 21:31:46] INFO:     Latency: 3840.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 3840.000000
[2025-11-17 21:31:46] INFO: 
  wq
[2025-11-17 21:31:46] INFO:     Op: matmul
[2025-11-17 21:31:46] INFO:     Shape: 6144x6144 (batch=64)
[2025-11-17 21:31:46] INFO:     Latency: 66127.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 69967.000000
[2025-11-17 21:31:46] INFO: 
  wk
[2025-11-17 21:31:46] INFO:     Op: matmul
[2025-11-17 21:31:46] INFO:     Shape: 6144x6144 (batch=64)
[2025-11-17 21:31:46] INFO:     Latency: 66127.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 136094.000000
[2025-11-17 21:31:46] INFO: 
  wv
[2025-11-17 21:31:46] INFO:     Op: matmul
[2025-11-17 21:31:46] INFO:     Shape: 6144x6144 (batch=64)
[2025-11-17 21:31:46] INFO:     Latency: 66127.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 202221.000000
[2025-11-17 21:31:46] INFO: 
  qk_matmul
[2025-11-17 21:31:46] INFO:     Op: batched_matmul
[2025-11-17 21:31:46] INFO:     Output Shape: torch.Size([64, 48, 1, 2048])
[2025-11-17 21:31:46] INFO:     Latency: 63910.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 266131.000000
[2025-11-17 21:31:46] INFO: 
  softmax
[2025-11-17 21:31:46] INFO:     Op: softmax
[2025-11-17 21:31:46] INFO:     Output Shape: torch.Size([64, 48, 1, 2048])
[2025-11-17 21:31:46] INFO:     Latency: 73728.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 339859.000000
[2025-11-17 21:31:46] INFO: 
  score_v_matmul
[2025-11-17 21:31:46] INFO:     Op: batched_matmul
[2025-11-17 21:31:46] INFO:     Output Shape: torch.Size([64, 48, 1, 128])
[2025-11-17 21:31:46] INFO:     Latency: 63910.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 403769.000000
[2025-11-17 21:31:46] INFO: 
  wo
[2025-11-17 21:31:46] INFO:     Op: matmul
[2025-11-17 21:31:46] INFO:     Shape: 6144x6144 (batch=64)
[2025-11-17 21:31:46] INFO:     Latency: 66127.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 469896.000000
[2025-11-17 21:31:46] INFO: 
  ffn_norm
[2025-11-17 21:31:46] INFO:     Op: rmsnorm
[2025-11-17 21:31:46] INFO:     Output Shape: torch.Size([64, 1, 6144])
[2025-11-17 21:31:46] INFO:     Latency: 3840.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 473736.000000
[2025-11-17 21:31:46] INFO: 
  w1
[2025-11-17 21:31:46] INFO:     Op: matmul
[2025-11-17 21:31:46] INFO:     Shape: 6144x23040 (batch=64)
[2025-11-17 21:31:46] INFO:     Latency: 143998.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 617734.000000
[2025-11-17 21:31:46] INFO: 
  w3
[2025-11-17 21:31:46] INFO:     Op: matmul
[2025-11-17 21:31:46] INFO:     Shape: 6144x23040 (batch=64)
[2025-11-17 21:31:46] INFO:     Latency: 143998.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 761732.000000
[2025-11-17 21:31:46] INFO: 
  silu
[2025-11-17 21:31:46] INFO:     Op: silu
[2025-11-17 21:31:46] INFO:     Output Shape: torch.Size([64, 1, 23040])
[2025-11-17 21:31:46] INFO:     Latency: 11520.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 773252.000000
[2025-11-17 21:31:46] INFO: 
  mul
[2025-11-17 21:31:46] INFO:     Op: vector_mul
[2025-11-17 21:31:46] INFO:     Output Shape: torch.Size([64, 1, 23040])
[2025-11-17 21:31:46] INFO:     Latency: 2880.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 776132.000000
[2025-11-17 21:31:46] INFO: 
  w2
[2025-11-17 21:31:46] INFO:     Op: matmul
[2025-11-17 21:31:46] INFO:     Shape: 23040x6144 (batch=64)
[2025-11-17 21:31:46] INFO:     Latency: 239308.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 1015440.000000
[2025-11-17 21:31:46] INFO: [19/36] ✓ Yi-34B batch=16 strategy=h2llm latency=266267.000000s
[2025-11-17 21:31:46] INFO: 
================================================================================
[2025-11-17 21:31:46] INFO: Detailed Latency: Yi-34B batch=16 strategy=h2llm
[2025-11-17 21:31:46] INFO: ================================================================================
[2025-11-17 21:31:46] INFO: 
  attention_norm
[2025-11-17 21:31:46] INFO:     Op: rmsnorm
[2025-11-17 21:31:46] INFO:     Output Shape: torch.Size([16, 1, 7168])
[2025-11-17 21:31:46] INFO:     Latency: 1120.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 1120.000000
[2025-11-17 21:31:46] INFO: 
  wq
[2025-11-17 21:31:46] INFO:     Op: matmul
[2025-11-17 21:31:46] INFO:     Shape: 7168x7168 (batch=16)
[2025-11-17 21:31:46] INFO:     Latency: 17142.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 18262.000000
[2025-11-17 21:31:46] INFO: 
  wk
[2025-11-17 21:31:46] INFO:     Op: matmul
[2025-11-17 21:31:46] INFO:     Shape: 7168x7168 (batch=16)
[2025-11-17 21:31:46] INFO:     Latency: 17142.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 35404.000000
[2025-11-17 21:31:46] INFO: 
  wv
[2025-11-17 21:31:46] INFO:     Op: matmul
[2025-11-17 21:31:46] INFO:     Shape: 7168x7168 (batch=16)
[2025-11-17 21:31:46] INFO:     Latency: 17142.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 52546.000000
[2025-11-17 21:31:46] INFO: 
  qk_matmul
[2025-11-17 21:31:46] INFO:     Op: batched_matmul
[2025-11-17 21:31:46] INFO:     Output Shape: torch.Size([16, 56, 1, 2048])
[2025-11-17 21:31:46] INFO:     Latency: 23342.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 75888.000000
[2025-11-17 21:31:46] INFO: 
  softmax
[2025-11-17 21:31:46] INFO:     Op: softmax
[2025-11-17 21:31:46] INFO:     Output Shape: torch.Size([16, 56, 1, 2048])
[2025-11-17 21:31:46] INFO:     Latency: 21504.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 97392.000000
[2025-11-17 21:31:46] INFO: 
  score_v_matmul
[2025-11-17 21:31:46] INFO:     Op: batched_matmul
[2025-11-17 21:31:46] INFO:     Output Shape: torch.Size([16, 56, 1, 128])
[2025-11-17 21:31:46] INFO:     Latency: 23342.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 120734.000000
[2025-11-17 21:31:46] INFO: 
  wo
[2025-11-17 21:31:46] INFO:     Op: matmul
[2025-11-17 21:31:46] INFO:     Shape: 7168x7168 (batch=16)
[2025-11-17 21:31:46] INFO:     Latency: 17142.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 137876.000000
[2025-11-17 21:31:46] INFO: 
  ffn_norm
[2025-11-17 21:31:46] INFO:     Op: rmsnorm
[2025-11-17 21:31:46] INFO:     Output Shape: torch.Size([16, 1, 7168])
[2025-11-17 21:31:46] INFO:     Latency: 1120.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 138996.000000
[2025-11-17 21:31:46] INFO: 
  w1
[2025-11-17 21:31:46] INFO:     Op: matmul
[2025-11-17 21:31:46] INFO:     Shape: 7168x20480 (batch=16)
[2025-11-17 21:31:46] INFO:     Latency: 41357.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 180353.000000
[2025-11-17 21:31:46] INFO: 
  w3
[2025-11-17 21:31:46] INFO:     Op: matmul
[2025-11-17 21:31:46] INFO:     Shape: 7168x20480 (batch=16)
[2025-11-17 21:31:46] INFO:     Latency: 41357.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 221710.000000
[2025-11-17 21:31:46] INFO: 
  silu
[2025-11-17 21:31:46] INFO:     Op: silu
[2025-11-17 21:31:46] INFO:     Output Shape: torch.Size([16, 1, 20480])
[2025-11-17 21:31:46] INFO:     Latency: 2560.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 224270.000000
[2025-11-17 21:31:46] INFO: 
  mul
[2025-11-17 21:31:46] INFO:     Op: vector_mul
[2025-11-17 21:31:46] INFO:     Output Shape: torch.Size([16, 1, 20480])
[2025-11-17 21:31:46] INFO:     Latency: 640.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 224910.000000
[2025-11-17 21:31:46] INFO: 
  w2
[2025-11-17 21:31:46] INFO:     Op: matmul
[2025-11-17 21:31:46] INFO:     Shape: 20480x7168 (batch=16)
[2025-11-17 21:31:46] INFO:     Latency: 41357.000000
[2025-11-17 21:31:46] INFO:     Cumulative: 266267.000000
[2025-11-17 21:32:45] INFO: [20/36] ✓ Yi-34B batch=16 strategy=recursive_grid_search latency=193908.000000s
[2025-11-17 21:32:45] INFO: 
================================================================================
[2025-11-17 21:32:45] INFO: Detailed Latency: Yi-34B batch=16 strategy=recursive_grid_search
[2025-11-17 21:32:45] INFO: ================================================================================
[2025-11-17 21:32:45] INFO: 
  fused_matrix_2370469355904
[2025-11-17 21:32:45] INFO:     Op: fusion_matrix
[2025-11-17 21:32:45] INFO:     Shape: 7168x21504 (batch=16)
[2025-11-17 21:32:45] INFO:     Latency: 36041.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 36041.000000
[2025-11-17 21:32:45] INFO: 
  qk_matmul
[2025-11-17 21:32:45] INFO:     Op: batched_matmul
[2025-11-17 21:32:45] INFO:     Output Shape: torch.Size([16, 56, 1, 2048])
[2025-11-17 21:32:45] INFO:     Latency: 18601.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 54642.000000
[2025-11-17 21:32:45] INFO: 
  score_v_matmul
[2025-11-17 21:32:45] INFO:     Op: batched_matmul
[2025-11-17 21:32:45] INFO:     Output Shape: torch.Size([16, 56, 1, 128])
[2025-11-17 21:32:45] INFO:     Latency: 23408.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 78050.000000
[2025-11-17 21:32:45] INFO: 
  wo
[2025-11-17 21:32:45] INFO:     Op: matmul
[2025-11-17 21:32:45] INFO:     Shape: 7168x7168 (batch=16)
[2025-11-17 21:32:45] INFO:     Latency: 13366.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 91416.000000
[2025-11-17 21:32:45] INFO: 
  fused_matrix_2370469353456
[2025-11-17 21:32:45] INFO:     Op: fusion_matrix
[2025-11-17 21:32:45] INFO:     Shape: 7168x40960 (batch=16)
[2025-11-17 21:32:45] INFO:     Latency: 67383.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 158799.000000
[2025-11-17 21:32:45] INFO: 
  w2
[2025-11-17 21:32:45] INFO:     Op: matmul
[2025-11-17 21:32:45] INFO:     Shape: 20480x7168 (batch=16)
[2025-11-17 21:32:45] INFO:     Latency: 35109.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 193908.000000
[2025-11-17 21:32:45] INFO: [21/36] ✓ Yi-34B batch=16 strategy=trivial latency=315824.000000s
[2025-11-17 21:32:45] INFO: 
================================================================================
[2025-11-17 21:32:45] INFO: Detailed Latency: Yi-34B batch=16 strategy=trivial
[2025-11-17 21:32:45] INFO: ================================================================================
[2025-11-17 21:32:45] INFO: 
  attention_norm
[2025-11-17 21:32:45] INFO:     Op: rmsnorm
[2025-11-17 21:32:45] INFO:     Output Shape: torch.Size([16, 1, 7168])
[2025-11-17 21:32:45] INFO:     Latency: 1120.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 1120.000000
[2025-11-17 21:32:45] INFO: 
  wq
[2025-11-17 21:32:45] INFO:     Op: matmul
[2025-11-17 21:32:45] INFO:     Shape: 7168x7168 (batch=16)
[2025-11-17 21:32:45] INFO:     Latency: 22665.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 23785.000000
[2025-11-17 21:32:45] INFO: 
  wk
[2025-11-17 21:32:45] INFO:     Op: matmul
[2025-11-17 21:32:45] INFO:     Shape: 7168x7168 (batch=16)
[2025-11-17 21:32:45] INFO:     Latency: 22665.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 46450.000000
[2025-11-17 21:32:45] INFO: 
  wv
[2025-11-17 21:32:45] INFO:     Op: matmul
[2025-11-17 21:32:45] INFO:     Shape: 7168x7168 (batch=16)
[2025-11-17 21:32:45] INFO:     Latency: 22665.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 69115.000000
[2025-11-17 21:32:45] INFO: 
  qk_matmul
[2025-11-17 21:32:45] INFO:     Op: batched_matmul
[2025-11-17 21:32:45] INFO:     Output Shape: torch.Size([16, 56, 1, 2048])
[2025-11-17 21:32:45] INFO:     Latency: 23342.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 92457.000000
[2025-11-17 21:32:45] INFO: 
  softmax
[2025-11-17 21:32:45] INFO:     Op: softmax
[2025-11-17 21:32:45] INFO:     Output Shape: torch.Size([16, 56, 1, 2048])
[2025-11-17 21:32:45] INFO:     Latency: 21504.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 113961.000000
[2025-11-17 21:32:45] INFO: 
  score_v_matmul
[2025-11-17 21:32:45] INFO:     Op: batched_matmul
[2025-11-17 21:32:45] INFO:     Output Shape: torch.Size([16, 56, 1, 128])
[2025-11-17 21:32:45] INFO:     Latency: 23342.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 137303.000000
[2025-11-17 21:32:45] INFO: 
  wo
[2025-11-17 21:32:45] INFO:     Op: matmul
[2025-11-17 21:32:45] INFO:     Shape: 7168x7168 (batch=16)
[2025-11-17 21:32:45] INFO:     Latency: 22665.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 159968.000000
[2025-11-17 21:32:45] INFO: 
  ffn_norm
[2025-11-17 21:32:45] INFO:     Op: rmsnorm
[2025-11-17 21:32:45] INFO:     Output Shape: torch.Size([16, 1, 7168])
[2025-11-17 21:32:45] INFO:     Latency: 1120.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 161088.000000
[2025-11-17 21:32:45] INFO: 
  w1
[2025-11-17 21:32:45] INFO:     Op: matmul
[2025-11-17 21:32:45] INFO:     Shape: 7168x20480 (batch=16)
[2025-11-17 21:32:45] INFO:     Latency: 44245.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 205333.000000
[2025-11-17 21:32:45] INFO: 
  w3
[2025-11-17 21:32:45] INFO:     Op: matmul
[2025-11-17 21:32:45] INFO:     Shape: 7168x20480 (batch=16)
[2025-11-17 21:32:45] INFO:     Latency: 44245.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 249578.000000
[2025-11-17 21:32:45] INFO: 
  silu
[2025-11-17 21:32:45] INFO:     Op: silu
[2025-11-17 21:32:45] INFO:     Output Shape: torch.Size([16, 1, 20480])
[2025-11-17 21:32:45] INFO:     Latency: 2560.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 252138.000000
[2025-11-17 21:32:45] INFO: 
  mul
[2025-11-17 21:32:45] INFO:     Op: vector_mul
[2025-11-17 21:32:45] INFO:     Output Shape: torch.Size([16, 1, 20480])
[2025-11-17 21:32:45] INFO:     Latency: 640.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 252778.000000
[2025-11-17 21:32:45] INFO: 
  w2
[2025-11-17 21:32:45] INFO:     Op: matmul
[2025-11-17 21:32:45] INFO:     Shape: 20480x7168 (batch=16)
[2025-11-17 21:32:45] INFO:     Latency: 63046.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 315824.000000
[2025-11-17 21:32:45] INFO: [22/36] ✓ Yi-34B batch=32 strategy=h2llm latency=478687.000000s
[2025-11-17 21:32:45] INFO: 
================================================================================
[2025-11-17 21:32:45] INFO: Detailed Latency: Yi-34B batch=32 strategy=h2llm
[2025-11-17 21:32:45] INFO: ================================================================================
[2025-11-17 21:32:45] INFO: 
  attention_norm
[2025-11-17 21:32:45] INFO:     Op: rmsnorm
[2025-11-17 21:32:45] INFO:     Output Shape: torch.Size([32, 1, 7168])
[2025-11-17 21:32:45] INFO:     Latency: 2240.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 2240.000000
[2025-11-17 21:32:45] INFO: 
  wq
[2025-11-17 21:32:45] INFO:     Op: matmul
[2025-11-17 21:32:45] INFO:     Shape: 7168x7168 (batch=32)
[2025-11-17 21:32:45] INFO:     Latency: 30000.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 32240.000000
[2025-11-17 21:32:45] INFO: 
  wk
[2025-11-17 21:32:45] INFO:     Op: matmul
[2025-11-17 21:32:45] INFO:     Shape: 7168x7168 (batch=32)
[2025-11-17 21:32:45] INFO:     Latency: 30000.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 62240.000000
[2025-11-17 21:32:45] INFO: 
  wv
[2025-11-17 21:32:45] INFO:     Op: matmul
[2025-11-17 21:32:45] INFO:     Shape: 7168x7168 (batch=32)
[2025-11-17 21:32:45] INFO:     Latency: 30000.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 92240.000000
[2025-11-17 21:32:45] INFO: 
  qk_matmul
[2025-11-17 21:32:45] INFO:     Op: batched_matmul
[2025-11-17 21:32:45] INFO:     Output Shape: torch.Size([32, 56, 1, 2048])
[2025-11-17 21:32:45] INFO:     Latency: 46684.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 138924.000000
[2025-11-17 21:32:45] INFO: 
  softmax
[2025-11-17 21:32:45] INFO:     Op: softmax
[2025-11-17 21:32:45] INFO:     Output Shape: torch.Size([32, 56, 1, 2048])
[2025-11-17 21:32:45] INFO:     Latency: 43008.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 181932.000000
[2025-11-17 21:32:45] INFO: 
  score_v_matmul
[2025-11-17 21:32:45] INFO:     Op: batched_matmul
[2025-11-17 21:32:45] INFO:     Output Shape: torch.Size([32, 56, 1, 128])
[2025-11-17 21:32:45] INFO:     Latency: 46684.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 228616.000000
[2025-11-17 21:32:45] INFO: 
  wo
[2025-11-17 21:32:45] INFO:     Op: matmul
[2025-11-17 21:32:45] INFO:     Shape: 7168x7168 (batch=32)
[2025-11-17 21:32:45] INFO:     Latency: 30000.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 258616.000000
[2025-11-17 21:32:45] INFO: 
  ffn_norm
[2025-11-17 21:32:45] INFO:     Op: rmsnorm
[2025-11-17 21:32:45] INFO:     Output Shape: torch.Size([32, 1, 7168])
[2025-11-17 21:32:45] INFO:     Latency: 2240.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 260856.000000
[2025-11-17 21:32:45] INFO: 
  w1
[2025-11-17 21:32:45] INFO:     Op: matmul
[2025-11-17 21:32:45] INFO:     Shape: 7168x20480 (batch=32)
[2025-11-17 21:32:45] INFO:     Latency: 70477.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 331333.000000
[2025-11-17 21:32:45] INFO: 
  w3
[2025-11-17 21:32:45] INFO:     Op: matmul
[2025-11-17 21:32:45] INFO:     Shape: 7168x20480 (batch=32)
[2025-11-17 21:32:45] INFO:     Latency: 70477.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 401810.000000
[2025-11-17 21:32:45] INFO: 
  silu
[2025-11-17 21:32:45] INFO:     Op: silu
[2025-11-17 21:32:45] INFO:     Output Shape: torch.Size([32, 1, 20480])
[2025-11-17 21:32:45] INFO:     Latency: 5120.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 406930.000000
[2025-11-17 21:32:45] INFO: 
  mul
[2025-11-17 21:32:45] INFO:     Op: vector_mul
[2025-11-17 21:32:45] INFO:     Output Shape: torch.Size([32, 1, 20480])
[2025-11-17 21:32:45] INFO:     Latency: 1280.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 408210.000000
[2025-11-17 21:32:45] INFO: 
  w2
[2025-11-17 21:32:45] INFO:     Op: matmul
[2025-11-17 21:32:45] INFO:     Shape: 20480x7168 (batch=32)
[2025-11-17 21:32:45] INFO:     Latency: 70477.000000
[2025-11-17 21:32:45] INFO:     Cumulative: 478687.000000
[2025-11-17 21:33:00] INFO: [23/36] ✓ Yi-34B batch=32 strategy=recursive_grid_search latency=338765.000000s
[2025-11-17 21:33:00] INFO: 
================================================================================
[2025-11-17 21:33:00] INFO: Detailed Latency: Yi-34B batch=32 strategy=recursive_grid_search
[2025-11-17 21:33:00] INFO: ================================================================================
[2025-11-17 21:33:00] INFO: 
  fused_matrix_2182066698432
[2025-11-17 21:33:00] INFO:     Op: fusion_matrix
[2025-11-17 21:33:00] INFO:     Shape: 7168x21504 (batch=32)
[2025-11-17 21:33:00] INFO:     Latency: 61045.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 61045.000000
[2025-11-17 21:33:00] INFO: 
  qk_matmul
[2025-11-17 21:33:00] INFO:     Op: batched_matmul
[2025-11-17 21:33:00] INFO:     Output Shape: torch.Size([32, 56, 1, 2048])
[2025-11-17 21:33:00] INFO:     Latency: 37202.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 98247.000000
[2025-11-17 21:33:00] INFO: 
  score_v_matmul
[2025-11-17 21:33:00] INFO:     Op: batched_matmul
[2025-11-17 21:33:00] INFO:     Output Shape: torch.Size([32, 56, 1, 128])
[2025-11-17 21:33:00] INFO:     Latency: 46816.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 145063.000000
[2025-11-17 21:33:00] INFO: 
  wo
[2025-11-17 21:33:00] INFO:     Op: matmul
[2025-11-17 21:33:00] INFO:     Shape: 7168x7168 (batch=32)
[2025-11-17 21:33:00] INFO:     Latency: 23301.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 168364.000000
[2025-11-17 21:33:00] INFO: 
  fused_matrix_2182066695264
[2025-11-17 21:33:00] INFO:     Op: fusion_matrix
[2025-11-17 21:33:00] INFO:     Shape: 7168x40960 (batch=32)
[2025-11-17 21:33:00] INFO:     Latency: 111668.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 280032.000000
[2025-11-17 21:33:00] INFO: 
  w2
[2025-11-17 21:33:00] INFO:     Op: matmul
[2025-11-17 21:33:00] INFO:     Shape: 20480x7168 (batch=32)
[2025-11-17 21:33:00] INFO:     Latency: 58733.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 338765.000000
[2025-11-17 21:33:00] INFO: [24/36] ✓ Yi-34B batch=32 strategy=trivial latency=577775.000000s
[2025-11-17 21:33:00] INFO: 
================================================================================
[2025-11-17 21:33:00] INFO: Detailed Latency: Yi-34B batch=32 strategy=trivial
[2025-11-17 21:33:00] INFO: ================================================================================
[2025-11-17 21:33:00] INFO: 
  attention_norm
[2025-11-17 21:33:00] INFO:     Op: rmsnorm
[2025-11-17 21:33:00] INFO:     Output Shape: torch.Size([32, 1, 7168])
[2025-11-17 21:33:00] INFO:     Latency: 2240.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 2240.000000
[2025-11-17 21:33:00] INFO: 
  wq
[2025-11-17 21:33:00] INFO:     Op: matmul
[2025-11-17 21:33:00] INFO:     Shape: 7168x7168 (batch=32)
[2025-11-17 21:33:00] INFO:     Latency: 41042.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 43282.000000
[2025-11-17 21:33:00] INFO: 
  wk
[2025-11-17 21:33:00] INFO:     Op: matmul
[2025-11-17 21:33:00] INFO:     Shape: 7168x7168 (batch=32)
[2025-11-17 21:33:00] INFO:     Latency: 41042.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 84324.000000
[2025-11-17 21:33:00] INFO: 
  wv
[2025-11-17 21:33:00] INFO:     Op: matmul
[2025-11-17 21:33:00] INFO:     Shape: 7168x7168 (batch=32)
[2025-11-17 21:33:00] INFO:     Latency: 41042.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 125366.000000
[2025-11-17 21:33:00] INFO: 
  qk_matmul
[2025-11-17 21:33:00] INFO:     Op: batched_matmul
[2025-11-17 21:33:00] INFO:     Output Shape: torch.Size([32, 56, 1, 2048])
[2025-11-17 21:33:00] INFO:     Latency: 46684.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 172050.000000
[2025-11-17 21:33:00] INFO: 
  softmax
[2025-11-17 21:33:00] INFO:     Op: softmax
[2025-11-17 21:33:00] INFO:     Output Shape: torch.Size([32, 56, 1, 2048])
[2025-11-17 21:33:00] INFO:     Latency: 43008.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 215058.000000
[2025-11-17 21:33:00] INFO: 
  score_v_matmul
[2025-11-17 21:33:00] INFO:     Op: batched_matmul
[2025-11-17 21:33:00] INFO:     Output Shape: torch.Size([32, 56, 1, 128])
[2025-11-17 21:33:00] INFO:     Latency: 46684.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 261742.000000
[2025-11-17 21:33:00] INFO: 
  wo
[2025-11-17 21:33:00] INFO:     Op: matmul
[2025-11-17 21:33:00] INFO:     Shape: 7168x7168 (batch=32)
[2025-11-17 21:33:00] INFO:     Latency: 41042.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 302784.000000
[2025-11-17 21:33:00] INFO: 
  ffn_norm
[2025-11-17 21:33:00] INFO:     Op: rmsnorm
[2025-11-17 21:33:00] INFO:     Output Shape: torch.Size([32, 1, 7168])
[2025-11-17 21:33:00] INFO:     Latency: 2240.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 305024.000000
[2025-11-17 21:33:00] INFO: 
  w1
[2025-11-17 21:33:00] INFO:     Op: matmul
[2025-11-17 21:33:00] INFO:     Shape: 7168x20480 (batch=32)
[2025-11-17 21:33:00] INFO:     Latency: 76253.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 381277.000000
[2025-11-17 21:33:00] INFO: 
  w3
[2025-11-17 21:33:00] INFO:     Op: matmul
[2025-11-17 21:33:00] INFO:     Shape: 7168x20480 (batch=32)
[2025-11-17 21:33:00] INFO:     Latency: 76253.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 457530.000000
[2025-11-17 21:33:00] INFO: 
  silu
[2025-11-17 21:33:00] INFO:     Op: silu
[2025-11-17 21:33:00] INFO:     Output Shape: torch.Size([32, 1, 20480])
[2025-11-17 21:33:00] INFO:     Latency: 5120.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 462650.000000
[2025-11-17 21:33:00] INFO: 
  mul
[2025-11-17 21:33:00] INFO:     Op: vector_mul
[2025-11-17 21:33:00] INFO:     Output Shape: torch.Size([32, 1, 20480])
[2025-11-17 21:33:00] INFO:     Latency: 1280.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 463930.000000
[2025-11-17 21:33:00] INFO: 
  w2
[2025-11-17 21:33:00] INFO:     Op: matmul
[2025-11-17 21:33:00] INFO:     Shape: 20480x7168 (batch=32)
[2025-11-17 21:33:00] INFO:     Latency: 113845.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 577775.000000
[2025-11-17 21:33:00] INFO: [25/36] ✓ Yi-34B batch=64 strategy=h2llm latency=953127.000000s
[2025-11-17 21:33:00] INFO: 
================================================================================
[2025-11-17 21:33:00] INFO: Detailed Latency: Yi-34B batch=64 strategy=h2llm
[2025-11-17 21:33:00] INFO: ================================================================================
[2025-11-17 21:33:00] INFO: 
  attention_norm
[2025-11-17 21:33:00] INFO:     Op: rmsnorm
[2025-11-17 21:33:00] INFO:     Output Shape: torch.Size([64, 1, 7168])
[2025-11-17 21:33:00] INFO:     Latency: 4480.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 4480.000000
[2025-11-17 21:33:00] INFO: 
  wq
[2025-11-17 21:33:00] INFO:     Op: matmul
[2025-11-17 21:33:00] INFO:     Shape: 7168x7168 (batch=64)
[2025-11-17 21:33:00] INFO:     Latency: 60000.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 64480.000000
[2025-11-17 21:33:00] INFO: 
  wk
[2025-11-17 21:33:00] INFO:     Op: matmul
[2025-11-17 21:33:00] INFO:     Shape: 7168x7168 (batch=64)
[2025-11-17 21:33:00] INFO:     Latency: 60000.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 124480.000000
[2025-11-17 21:33:00] INFO: 
  wv
[2025-11-17 21:33:00] INFO:     Op: matmul
[2025-11-17 21:33:00] INFO:     Shape: 7168x7168 (batch=64)
[2025-11-17 21:33:00] INFO:     Latency: 60000.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 184480.000000
[2025-11-17 21:33:00] INFO: 
  qk_matmul
[2025-11-17 21:33:00] INFO:     Op: batched_matmul
[2025-11-17 21:33:00] INFO:     Output Shape: torch.Size([64, 56, 1, 2048])
[2025-11-17 21:33:00] INFO:     Latency: 91246.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 275726.000000
[2025-11-17 21:33:00] INFO: 
  softmax
[2025-11-17 21:33:00] INFO:     Op: softmax
[2025-11-17 21:33:00] INFO:     Output Shape: torch.Size([64, 56, 1, 2048])
[2025-11-17 21:33:00] INFO:     Latency: 86016.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 361742.000000
[2025-11-17 21:33:00] INFO: 
  score_v_matmul
[2025-11-17 21:33:00] INFO:     Op: batched_matmul
[2025-11-17 21:33:00] INFO:     Output Shape: torch.Size([64, 56, 1, 128])
[2025-11-17 21:33:00] INFO:     Latency: 91246.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 452988.000000
[2025-11-17 21:33:00] INFO: 
  wo
[2025-11-17 21:33:00] INFO:     Op: matmul
[2025-11-17 21:33:00] INFO:     Shape: 7168x7168 (batch=64)
[2025-11-17 21:33:00] INFO:     Latency: 60000.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 512988.000000
[2025-11-17 21:33:00] INFO: 
  ffn_norm
[2025-11-17 21:33:00] INFO:     Op: rmsnorm
[2025-11-17 21:33:00] INFO:     Output Shape: torch.Size([64, 1, 7168])
[2025-11-17 21:33:00] INFO:     Latency: 4480.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 517468.000000
[2025-11-17 21:33:00] INFO: 
  w1
[2025-11-17 21:33:00] INFO:     Op: matmul
[2025-11-17 21:33:00] INFO:     Shape: 7168x20480 (batch=64)
[2025-11-17 21:33:00] INFO:     Latency: 140953.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 658421.000000
[2025-11-17 21:33:00] INFO: 
  w3
[2025-11-17 21:33:00] INFO:     Op: matmul
[2025-11-17 21:33:00] INFO:     Shape: 7168x20480 (batch=64)
[2025-11-17 21:33:00] INFO:     Latency: 140953.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 799374.000000
[2025-11-17 21:33:00] INFO: 
  silu
[2025-11-17 21:33:00] INFO:     Op: silu
[2025-11-17 21:33:00] INFO:     Output Shape: torch.Size([64, 1, 20480])
[2025-11-17 21:33:00] INFO:     Latency: 10240.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 809614.000000
[2025-11-17 21:33:00] INFO: 
  mul
[2025-11-17 21:33:00] INFO:     Op: vector_mul
[2025-11-17 21:33:00] INFO:     Output Shape: torch.Size([64, 1, 20480])
[2025-11-17 21:33:00] INFO:     Latency: 2560.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 812174.000000
[2025-11-17 21:33:00] INFO: 
  w2
[2025-11-17 21:33:00] INFO:     Op: matmul
[2025-11-17 21:33:00] INFO:     Shape: 20480x7168 (batch=64)
[2025-11-17 21:33:00] INFO:     Latency: 140953.000000
[2025-11-17 21:33:00] INFO:     Cumulative: 953127.000000
[2025-11-17 21:34:00] INFO: [26/36] ✓ Yi-34B batch=64 strategy=recursive_grid_search latency=673711.000000s
[2025-11-17 21:34:00] INFO: 
================================================================================
[2025-11-17 21:34:00] INFO: Detailed Latency: Yi-34B batch=64 strategy=recursive_grid_search
[2025-11-17 21:34:00] INFO: ================================================================================
[2025-11-17 21:34:00] INFO: 
  fused_matrix_2948724022288
[2025-11-17 21:34:00] INFO:     Op: fusion_matrix
[2025-11-17 21:34:00] INFO:     Shape: 7168x21504 (batch=64)
[2025-11-17 21:34:00] INFO:     Latency: 122093.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 122093.000000
[2025-11-17 21:34:00] INFO: 
  qk_matmul
[2025-11-17 21:34:00] INFO:     Op: batched_matmul
[2025-11-17 21:34:00] INFO:     Output Shape: torch.Size([64, 56, 1, 2048])
[2025-11-17 21:34:00] INFO:     Latency: 72713.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 194806.000000
[2025-11-17 21:34:00] INFO: 
  score_v_matmul
[2025-11-17 21:34:00] INFO:     Op: batched_matmul
[2025-11-17 21:34:00] INFO:     Output Shape: torch.Size([64, 56, 1, 128])
[2025-11-17 21:34:00] INFO:     Latency: 91504.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 286310.000000
[2025-11-17 21:34:00] INFO: 
  wo
[2025-11-17 21:34:00] INFO:     Op: matmul
[2025-11-17 21:34:00] INFO:     Shape: 7168x7168 (batch=64)
[2025-11-17 21:34:00] INFO:     Latency: 46600.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 332910.000000
[2025-11-17 21:34:00] INFO: 
  fused_matrix_2948724018544
[2025-11-17 21:34:00] INFO:     Op: fusion_matrix
[2025-11-17 21:34:00] INFO:     Shape: 7168x40960 (batch=64)
[2025-11-17 21:34:00] INFO:     Latency: 223336.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 556246.000000
[2025-11-17 21:34:00] INFO: 
  w2
[2025-11-17 21:34:00] INFO:     Op: matmul
[2025-11-17 21:34:00] INFO:     Shape: 20480x7168 (batch=64)
[2025-11-17 21:34:00] INFO:     Latency: 117465.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 673711.000000
[2025-11-17 21:34:00] INFO: [27/36] ✓ Yi-34B batch=64 strategy=trivial latency=1151294.000000s
[2025-11-17 21:34:00] INFO: 
================================================================================
[2025-11-17 21:34:00] INFO: Detailed Latency: Yi-34B batch=64 strategy=trivial
[2025-11-17 21:34:00] INFO: ================================================================================
[2025-11-17 21:34:00] INFO: 
  attention_norm
[2025-11-17 21:34:00] INFO:     Op: rmsnorm
[2025-11-17 21:34:00] INFO:     Output Shape: torch.Size([64, 1, 7168])
[2025-11-17 21:34:00] INFO:     Latency: 4480.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 4480.000000
[2025-11-17 21:34:00] INFO: 
  wq
[2025-11-17 21:34:00] INFO:     Op: matmul
[2025-11-17 21:34:00] INFO:     Shape: 7168x7168 (batch=64)
[2025-11-17 21:34:00] INFO:     Latency: 82083.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 86563.000000
[2025-11-17 21:34:00] INFO: 
  wk
[2025-11-17 21:34:00] INFO:     Op: matmul
[2025-11-17 21:34:00] INFO:     Shape: 7168x7168 (batch=64)
[2025-11-17 21:34:00] INFO:     Latency: 82083.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 168646.000000
[2025-11-17 21:34:00] INFO: 
  wv
[2025-11-17 21:34:00] INFO:     Op: matmul
[2025-11-17 21:34:00] INFO:     Shape: 7168x7168 (batch=64)
[2025-11-17 21:34:00] INFO:     Latency: 82083.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 250729.000000
[2025-11-17 21:34:00] INFO: 
  qk_matmul
[2025-11-17 21:34:00] INFO:     Op: batched_matmul
[2025-11-17 21:34:00] INFO:     Output Shape: torch.Size([64, 56, 1, 2048])
[2025-11-17 21:34:00] INFO:     Latency: 91246.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 341975.000000
[2025-11-17 21:34:00] INFO: 
  softmax
[2025-11-17 21:34:00] INFO:     Op: softmax
[2025-11-17 21:34:00] INFO:     Output Shape: torch.Size([64, 56, 1, 2048])
[2025-11-17 21:34:00] INFO:     Latency: 86016.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 427991.000000
[2025-11-17 21:34:00] INFO: 
  score_v_matmul
[2025-11-17 21:34:00] INFO:     Op: batched_matmul
[2025-11-17 21:34:00] INFO:     Output Shape: torch.Size([64, 56, 1, 128])
[2025-11-17 21:34:00] INFO:     Latency: 91246.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 519237.000000
[2025-11-17 21:34:00] INFO: 
  wo
[2025-11-17 21:34:00] INFO:     Op: matmul
[2025-11-17 21:34:00] INFO:     Shape: 7168x7168 (batch=64)
[2025-11-17 21:34:00] INFO:     Latency: 82083.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 601320.000000
[2025-11-17 21:34:00] INFO: 
  ffn_norm
[2025-11-17 21:34:00] INFO:     Op: rmsnorm
[2025-11-17 21:34:00] INFO:     Output Shape: torch.Size([64, 1, 7168])
[2025-11-17 21:34:00] INFO:     Latency: 4480.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 605800.000000
[2025-11-17 21:34:00] INFO: 
  w1
[2025-11-17 21:34:00] INFO:     Op: matmul
[2025-11-17 21:34:00] INFO:     Shape: 7168x20480 (batch=64)
[2025-11-17 21:34:00] INFO:     Latency: 152503.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 758303.000000
[2025-11-17 21:34:00] INFO: 
  w3
[2025-11-17 21:34:00] INFO:     Op: matmul
[2025-11-17 21:34:00] INFO:     Shape: 7168x20480 (batch=64)
[2025-11-17 21:34:00] INFO:     Latency: 152503.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 910806.000000
[2025-11-17 21:34:00] INFO: 
  silu
[2025-11-17 21:34:00] INFO:     Op: silu
[2025-11-17 21:34:00] INFO:     Output Shape: torch.Size([64, 1, 20480])
[2025-11-17 21:34:00] INFO:     Latency: 10240.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 921046.000000
[2025-11-17 21:34:00] INFO: 
  mul
[2025-11-17 21:34:00] INFO:     Op: vector_mul
[2025-11-17 21:34:00] INFO:     Output Shape: torch.Size([64, 1, 20480])
[2025-11-17 21:34:00] INFO:     Latency: 2560.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 923606.000000
[2025-11-17 21:34:00] INFO: 
  w2
[2025-11-17 21:34:00] INFO:     Op: matmul
[2025-11-17 21:34:00] INFO:     Shape: 20480x7168 (batch=64)
[2025-11-17 21:34:00] INFO:     Latency: 227688.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 1151294.000000
[2025-11-17 21:34:00] INFO: [28/36] ✓ Meta-Llama-3-70B batch=16 strategy=h2llm latency=356018.000000s
[2025-11-17 21:34:00] INFO: 
================================================================================
[2025-11-17 21:34:00] INFO: Detailed Latency: Meta-Llama-3-70B batch=16 strategy=h2llm
[2025-11-17 21:34:00] INFO: ================================================================================
[2025-11-17 21:34:00] INFO: 
  attention_norm
[2025-11-17 21:34:00] INFO:     Op: rmsnorm
[2025-11-17 21:34:00] INFO:     Output Shape: torch.Size([16, 1, 8192])
[2025-11-17 21:34:00] INFO:     Latency: 1280.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 1280.000000
[2025-11-17 21:34:00] INFO: 
  wq
[2025-11-17 21:34:00] INFO:     Op: matmul
[2025-11-17 21:34:00] INFO:     Shape: 8192x8192 (batch=16)
[2025-11-17 21:34:00] INFO:     Latency: 21336.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 22616.000000
[2025-11-17 21:34:00] INFO: 
  wk
[2025-11-17 21:34:00] INFO:     Op: matmul
[2025-11-17 21:34:00] INFO:     Shape: 8192x8192 (batch=16)
[2025-11-17 21:34:00] INFO:     Latency: 21336.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 43952.000000
[2025-11-17 21:34:00] INFO: 
  wv
[2025-11-17 21:34:00] INFO:     Op: matmul
[2025-11-17 21:34:00] INFO:     Shape: 8192x8192 (batch=16)
[2025-11-17 21:34:00] INFO:     Latency: 21336.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 65288.000000
[2025-11-17 21:34:00] INFO: 
  qk_matmul
[2025-11-17 21:34:00] INFO:     Op: batched_matmul
[2025-11-17 21:34:00] INFO:     Output Shape: torch.Size([16, 64, 1, 2048])
[2025-11-17 21:34:00] INFO:     Latency: 25641.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 90929.000000
[2025-11-17 21:34:00] INFO: 
  softmax
[2025-11-17 21:34:00] INFO:     Op: softmax
[2025-11-17 21:34:00] INFO:     Output Shape: torch.Size([16, 64, 1, 2048])
[2025-11-17 21:34:00] INFO:     Latency: 24576.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 115505.000000
[2025-11-17 21:34:00] INFO: 
  score_v_matmul
[2025-11-17 21:34:00] INFO:     Op: batched_matmul
[2025-11-17 21:34:00] INFO:     Output Shape: torch.Size([16, 64, 1, 128])
[2025-11-17 21:34:00] INFO:     Latency: 25641.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 141146.000000
[2025-11-17 21:34:00] INFO: 
  wo
[2025-11-17 21:34:00] INFO:     Op: matmul
[2025-11-17 21:34:00] INFO:     Shape: 8192x8192 (batch=16)
[2025-11-17 21:34:00] INFO:     Latency: 21336.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 162482.000000
[2025-11-17 21:34:00] INFO: 
  ffn_norm
[2025-11-17 21:34:00] INFO:     Op: rmsnorm
[2025-11-17 21:34:00] INFO:     Output Shape: torch.Size([16, 1, 8192])
[2025-11-17 21:34:00] INFO:     Latency: 1280.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 163762.000000
[2025-11-17 21:34:00] INFO: 
  w1
[2025-11-17 21:34:00] INFO:     Op: matmul
[2025-11-17 21:34:00] INFO:     Shape: 8192x28672 (batch=16)
[2025-11-17 21:34:00] INFO:     Latency: 62592.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 226354.000000
[2025-11-17 21:34:00] INFO: 
  w3
[2025-11-17 21:34:00] INFO:     Op: matmul
[2025-11-17 21:34:00] INFO:     Shape: 8192x28672 (batch=16)
[2025-11-17 21:34:00] INFO:     Latency: 62592.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 288946.000000
[2025-11-17 21:34:00] INFO: 
  silu
[2025-11-17 21:34:00] INFO:     Op: silu
[2025-11-17 21:34:00] INFO:     Output Shape: torch.Size([16, 1, 28672])
[2025-11-17 21:34:00] INFO:     Latency: 3584.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 292530.000000
[2025-11-17 21:34:00] INFO: 
  mul
[2025-11-17 21:34:00] INFO:     Op: vector_mul
[2025-11-17 21:34:00] INFO:     Output Shape: torch.Size([16, 1, 28672])
[2025-11-17 21:34:00] INFO:     Latency: 896.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 293426.000000
[2025-11-17 21:34:00] INFO: 
  w2
[2025-11-17 21:34:00] INFO:     Op: matmul
[2025-11-17 21:34:00] INFO:     Shape: 28672x8192 (batch=16)
[2025-11-17 21:34:00] INFO:     Latency: 62592.000000
[2025-11-17 21:34:00] INFO:     Cumulative: 356018.000000
[2025-11-17 21:35:31] INFO: [29/36] ✓ Meta-Llama-3-70B batch=16 strategy=recursive_grid_search latency=268537.000000s
[2025-11-17 21:35:31] INFO: 
================================================================================
[2025-11-17 21:35:31] INFO: Detailed Latency: Meta-Llama-3-70B batch=16 strategy=recursive_grid_search
[2025-11-17 21:35:31] INFO: ================================================================================
[2025-11-17 21:35:31] INFO: 
  fused_matrix_2368920936640
[2025-11-17 21:35:31] INFO:     Op: fusion_matrix
[2025-11-17 21:35:31] INFO:     Shape: 8192x24576 (batch=16)
[2025-11-17 21:35:31] INFO:     Latency: 45684.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 45684.000000
[2025-11-17 21:35:31] INFO: 
  qk_matmul
[2025-11-17 21:35:31] INFO:     Op: batched_matmul
[2025-11-17 21:35:31] INFO:     Output Shape: torch.Size([16, 64, 1, 2048])
[2025-11-17 21:35:31] INFO:     Latency: 20922.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 66606.000000
[2025-11-17 21:35:31] INFO: 
  score_v_matmul
[2025-11-17 21:35:31] INFO:     Op: batched_matmul
[2025-11-17 21:35:31] INFO:     Output Shape: torch.Size([16, 64, 1, 128])
[2025-11-17 21:35:31] INFO:     Latency: 25718.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 92324.000000
[2025-11-17 21:35:31] INFO: 
  wo
[2025-11-17 21:35:31] INFO:     Op: matmul
[2025-11-17 21:35:31] INFO:     Shape: 8192x8192 (batch=16)
[2025-11-17 21:35:31] INFO:     Latency: 16638.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 108962.000000
[2025-11-17 21:35:31] INFO: 
  fused_matrix_2368920846464
[2025-11-17 21:35:31] INFO:     Op: fusion_matrix
[2025-11-17 21:35:31] INFO:     Shape: 8192x57344 (batch=16)
[2025-11-17 21:35:31] INFO:     Latency: 104467.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 213429.000000
[2025-11-17 21:35:31] INFO: 
  w2
[2025-11-17 21:35:31] INFO:     Op: matmul
[2025-11-17 21:35:31] INFO:     Shape: 28672x8192 (batch=16)
[2025-11-17 21:35:31] INFO:     Latency: 55108.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 268537.000000
[2025-11-17 21:35:31] INFO: [30/36] ✓ Meta-Llama-3-70B batch=16 strategy=trivial latency=418048.000000s
[2025-11-17 21:35:31] INFO: 
================================================================================
[2025-11-17 21:35:31] INFO: Detailed Latency: Meta-Llama-3-70B batch=16 strategy=trivial
[2025-11-17 21:35:31] INFO: ================================================================================
[2025-11-17 21:35:31] INFO: 
  attention_norm
[2025-11-17 21:35:31] INFO:     Op: rmsnorm
[2025-11-17 21:35:31] INFO:     Output Shape: torch.Size([16, 1, 8192])
[2025-11-17 21:35:31] INFO:     Latency: 1280.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 1280.000000
[2025-11-17 21:35:31] INFO: 
  wq
[2025-11-17 21:35:31] INFO:     Op: matmul
[2025-11-17 21:35:31] INFO:     Shape: 8192x8192 (batch=16)
[2025-11-17 21:35:31] INFO:     Latency: 27643.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 28923.000000
[2025-11-17 21:35:31] INFO: 
  wk
[2025-11-17 21:35:31] INFO:     Op: matmul
[2025-11-17 21:35:31] INFO:     Shape: 8192x8192 (batch=16)
[2025-11-17 21:35:31] INFO:     Latency: 27643.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 56566.000000
[2025-11-17 21:35:31] INFO: 
  wv
[2025-11-17 21:35:31] INFO:     Op: matmul
[2025-11-17 21:35:31] INFO:     Shape: 8192x8192 (batch=16)
[2025-11-17 21:35:31] INFO:     Latency: 27643.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 84209.000000
[2025-11-17 21:35:31] INFO: 
  qk_matmul
[2025-11-17 21:35:31] INFO:     Op: batched_matmul
[2025-11-17 21:35:31] INFO:     Output Shape: torch.Size([16, 64, 1, 2048])
[2025-11-17 21:35:31] INFO:     Latency: 25641.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 109850.000000
[2025-11-17 21:35:31] INFO: 
  softmax
[2025-11-17 21:35:31] INFO:     Op: softmax
[2025-11-17 21:35:31] INFO:     Output Shape: torch.Size([16, 64, 1, 2048])
[2025-11-17 21:35:31] INFO:     Latency: 24576.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 134426.000000
[2025-11-17 21:35:31] INFO: 
  score_v_matmul
[2025-11-17 21:35:31] INFO:     Op: batched_matmul
[2025-11-17 21:35:31] INFO:     Output Shape: torch.Size([16, 64, 1, 128])
[2025-11-17 21:35:31] INFO:     Latency: 25641.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 160067.000000
[2025-11-17 21:35:31] INFO: 
  wo
[2025-11-17 21:35:31] INFO:     Op: matmul
[2025-11-17 21:35:31] INFO:     Shape: 8192x8192 (batch=16)
[2025-11-17 21:35:31] INFO:     Latency: 27643.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 187710.000000
[2025-11-17 21:35:31] INFO: 
  ffn_norm
[2025-11-17 21:35:31] INFO:     Op: rmsnorm
[2025-11-17 21:35:31] INFO:     Output Shape: torch.Size([16, 1, 8192])
[2025-11-17 21:35:31] INFO:     Latency: 1280.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 188990.000000
[2025-11-17 21:35:31] INFO: 
  w1
[2025-11-17 21:35:31] INFO:     Op: matmul
[2025-11-17 21:35:31] INFO:     Shape: 8192x28672 (batch=16)
[2025-11-17 21:35:31] INFO:     Latency: 65229.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 254219.000000
[2025-11-17 21:35:31] INFO: 
  w3
[2025-11-17 21:35:31] INFO:     Op: matmul
[2025-11-17 21:35:31] INFO:     Shape: 8192x28672 (batch=16)
[2025-11-17 21:35:31] INFO:     Latency: 65229.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 319448.000000
[2025-11-17 21:35:31] INFO: 
  silu
[2025-11-17 21:35:31] INFO:     Op: silu
[2025-11-17 21:35:31] INFO:     Output Shape: torch.Size([16, 1, 28672])
[2025-11-17 21:35:31] INFO:     Latency: 3584.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 323032.000000
[2025-11-17 21:35:31] INFO: 
  mul
[2025-11-17 21:35:31] INFO:     Op: vector_mul
[2025-11-17 21:35:31] INFO:     Output Shape: torch.Size([16, 1, 28672])
[2025-11-17 21:35:31] INFO:     Latency: 896.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 323928.000000
[2025-11-17 21:35:31] INFO: 
  w2
[2025-11-17 21:35:31] INFO:     Op: matmul
[2025-11-17 21:35:31] INFO:     Shape: 28672x8192 (batch=16)
[2025-11-17 21:35:31] INFO:     Latency: 94120.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 418048.000000
[2025-11-17 21:35:31] INFO: [31/36] ✓ Meta-Llama-3-70B batch=32 strategy=h2llm latency=630939.000000s
[2025-11-17 21:35:31] INFO: 
================================================================================
[2025-11-17 21:35:31] INFO: Detailed Latency: Meta-Llama-3-70B batch=32 strategy=h2llm
[2025-11-17 21:35:31] INFO: ================================================================================
[2025-11-17 21:35:31] INFO: 
  attention_norm
[2025-11-17 21:35:31] INFO:     Op: rmsnorm
[2025-11-17 21:35:31] INFO:     Output Shape: torch.Size([32, 1, 8192])
[2025-11-17 21:35:31] INFO:     Latency: 2560.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 2560.000000
[2025-11-17 21:35:31] INFO: 
  wq
[2025-11-17 21:35:31] INFO:     Op: matmul
[2025-11-17 21:35:31] INFO:     Shape: 8192x8192 (batch=32)
[2025-11-17 21:35:31] INFO:     Latency: 37079.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 39639.000000
[2025-11-17 21:35:31] INFO: 
  wk
[2025-11-17 21:35:31] INFO:     Op: matmul
[2025-11-17 21:35:31] INFO:     Shape: 8192x8192 (batch=32)
[2025-11-17 21:35:31] INFO:     Latency: 37079.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 76718.000000
[2025-11-17 21:35:31] INFO: 
  wv
[2025-11-17 21:35:31] INFO:     Op: matmul
[2025-11-17 21:35:31] INFO:     Shape: 8192x8192 (batch=32)
[2025-11-17 21:35:31] INFO:     Latency: 37079.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 113797.000000
[2025-11-17 21:35:31] INFO: 
  qk_matmul
[2025-11-17 21:35:31] INFO:     Op: batched_matmul
[2025-11-17 21:35:31] INFO:     Output Shape: torch.Size([32, 64, 1, 2048])
[2025-11-17 21:35:31] INFO:     Latency: 51282.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 165079.000000
[2025-11-17 21:35:31] INFO: 
  softmax
[2025-11-17 21:35:31] INFO:     Op: softmax
[2025-11-17 21:35:31] INFO:     Output Shape: torch.Size([32, 64, 1, 2048])
[2025-11-17 21:35:31] INFO:     Latency: 49152.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 214231.000000
[2025-11-17 21:35:31] INFO: 
  score_v_matmul
[2025-11-17 21:35:31] INFO:     Op: batched_matmul
[2025-11-17 21:35:31] INFO:     Output Shape: torch.Size([32, 64, 1, 128])
[2025-11-17 21:35:31] INFO:     Latency: 51282.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 265513.000000
[2025-11-17 21:35:31] INFO: 
  wo
[2025-11-17 21:35:31] INFO:     Op: matmul
[2025-11-17 21:35:31] INFO:     Shape: 8192x8192 (batch=32)
[2025-11-17 21:35:31] INFO:     Latency: 37079.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 302592.000000
[2025-11-17 21:35:31] INFO: 
  ffn_norm
[2025-11-17 21:35:31] INFO:     Op: rmsnorm
[2025-11-17 21:35:31] INFO:     Output Shape: torch.Size([32, 1, 8192])
[2025-11-17 21:35:31] INFO:     Latency: 2560.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 305152.000000
[2025-11-17 21:35:31] INFO: 
  w1
[2025-11-17 21:35:31] INFO:     Op: matmul
[2025-11-17 21:35:31] INFO:     Shape: 8192x28672 (batch=32)
[2025-11-17 21:35:31] INFO:     Latency: 105609.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 410761.000000
[2025-11-17 21:35:31] INFO: 
  w3
[2025-11-17 21:35:31] INFO:     Op: matmul
[2025-11-17 21:35:31] INFO:     Shape: 8192x28672 (batch=32)
[2025-11-17 21:35:31] INFO:     Latency: 105609.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 516370.000000
[2025-11-17 21:35:31] INFO: 
  silu
[2025-11-17 21:35:31] INFO:     Op: silu
[2025-11-17 21:35:31] INFO:     Output Shape: torch.Size([32, 1, 28672])
[2025-11-17 21:35:31] INFO:     Latency: 7168.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 523538.000000
[2025-11-17 21:35:31] INFO: 
  mul
[2025-11-17 21:35:31] INFO:     Op: vector_mul
[2025-11-17 21:35:31] INFO:     Output Shape: torch.Size([32, 1, 28672])
[2025-11-17 21:35:31] INFO:     Latency: 1792.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 525330.000000
[2025-11-17 21:35:31] INFO: 
  w2
[2025-11-17 21:35:31] INFO:     Op: matmul
[2025-11-17 21:35:31] INFO:     Shape: 28672x8192 (batch=32)
[2025-11-17 21:35:31] INFO:     Latency: 105609.000000
[2025-11-17 21:35:31] INFO:     Cumulative: 630939.000000
[2025-11-17 21:35:38] INFO: [32/36] ✓ Meta-Llama-3-70B batch=32 strategy=recursive_grid_search latency=463931.000000s
[2025-11-17 21:35:38] INFO: 
================================================================================
[2025-11-17 21:35:38] INFO: Detailed Latency: Meta-Llama-3-70B batch=32 strategy=recursive_grid_search
[2025-11-17 21:35:38] INFO: ================================================================================
[2025-11-17 21:35:38] INFO: 
  fused_matrix_2377850829328
[2025-11-17 21:35:38] INFO:     Op: fusion_matrix
[2025-11-17 21:35:38] INFO:     Shape: 8192x24576 (batch=32)
[2025-11-17 21:35:38] INFO:     Latency: 76968.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 76968.000000
[2025-11-17 21:35:38] INFO: 
  qk_matmul
[2025-11-17 21:35:38] INFO:     Op: batched_matmul
[2025-11-17 21:35:38] INFO:     Output Shape: torch.Size([32, 64, 1, 2048])
[2025-11-17 21:35:38] INFO:     Latency: 41844.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 118812.000000
[2025-11-17 21:35:38] INFO: 
  score_v_matmul
[2025-11-17 21:35:38] INFO:     Op: batched_matmul
[2025-11-17 21:35:38] INFO:     Output Shape: torch.Size([32, 64, 1, 128])
[2025-11-17 21:35:38] INFO:     Latency: 51436.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 170248.000000
[2025-11-17 21:35:38] INFO: 
  wo
[2025-11-17 21:35:38] INFO:     Op: matmul
[2025-11-17 21:35:38] INFO:     Shape: 8192x8192 (batch=32)
[2025-11-17 21:35:38] INFO:     Latency: 28843.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 199091.000000
[2025-11-17 21:35:38] INFO: 
  fused_matrix_2377851260128
[2025-11-17 21:35:38] INFO:     Op: fusion_matrix
[2025-11-17 21:35:38] INFO:     Shape: 8192x57344 (batch=32)
[2025-11-17 21:35:38] INFO:     Latency: 174221.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 373312.000000
[2025-11-17 21:35:38] INFO: 
  w2
[2025-11-17 21:35:38] INFO:     Op: matmul
[2025-11-17 21:35:38] INFO:     Shape: 28672x8192 (batch=32)
[2025-11-17 21:35:38] INFO:     Latency: 90619.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 463931.000000
[2025-11-17 21:35:38] INFO: [33/36] ✓ Meta-Llama-3-70B batch=32 strategy=trivial latency=754966.000000s
[2025-11-17 21:35:38] INFO: 
================================================================================
[2025-11-17 21:35:38] INFO: Detailed Latency: Meta-Llama-3-70B batch=32 strategy=trivial
[2025-11-17 21:35:38] INFO: ================================================================================
[2025-11-17 21:35:38] INFO: 
  attention_norm
[2025-11-17 21:35:38] INFO:     Op: rmsnorm
[2025-11-17 21:35:38] INFO:     Output Shape: torch.Size([32, 1, 8192])
[2025-11-17 21:35:38] INFO:     Latency: 2560.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 2560.000000
[2025-11-17 21:35:38] INFO: 
  wq
[2025-11-17 21:35:38] INFO:     Op: matmul
[2025-11-17 21:35:38] INFO:     Shape: 8192x8192 (batch=32)
[2025-11-17 21:35:38] INFO:     Latency: 49690.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 52250.000000
[2025-11-17 21:35:38] INFO: 
  wk
[2025-11-17 21:35:38] INFO:     Op: matmul
[2025-11-17 21:35:38] INFO:     Shape: 8192x8192 (batch=32)
[2025-11-17 21:35:38] INFO:     Latency: 49690.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 101940.000000
[2025-11-17 21:35:38] INFO: 
  wv
[2025-11-17 21:35:38] INFO:     Op: matmul
[2025-11-17 21:35:38] INFO:     Shape: 8192x8192 (batch=32)
[2025-11-17 21:35:38] INFO:     Latency: 49690.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 151630.000000
[2025-11-17 21:35:38] INFO: 
  qk_matmul
[2025-11-17 21:35:38] INFO:     Op: batched_matmul
[2025-11-17 21:35:38] INFO:     Output Shape: torch.Size([32, 64, 1, 2048])
[2025-11-17 21:35:38] INFO:     Latency: 51282.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 202912.000000
[2025-11-17 21:35:38] INFO: 
  softmax
[2025-11-17 21:35:38] INFO:     Op: softmax
[2025-11-17 21:35:38] INFO:     Output Shape: torch.Size([32, 64, 1, 2048])
[2025-11-17 21:35:38] INFO:     Latency: 49152.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 252064.000000
[2025-11-17 21:35:38] INFO: 
  score_v_matmul
[2025-11-17 21:35:38] INFO:     Op: batched_matmul
[2025-11-17 21:35:38] INFO:     Output Shape: torch.Size([32, 64, 1, 128])
[2025-11-17 21:35:38] INFO:     Latency: 51282.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 303346.000000
[2025-11-17 21:35:38] INFO: 
  wo
[2025-11-17 21:35:38] INFO:     Op: matmul
[2025-11-17 21:35:38] INFO:     Shape: 8192x8192 (batch=32)
[2025-11-17 21:35:38] INFO:     Latency: 49690.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 353036.000000
[2025-11-17 21:35:38] INFO: 
  ffn_norm
[2025-11-17 21:35:38] INFO:     Op: rmsnorm
[2025-11-17 21:35:38] INFO:     Output Shape: torch.Size([32, 1, 8192])
[2025-11-17 21:35:38] INFO:     Latency: 2560.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 355596.000000
[2025-11-17 21:35:38] INFO: 
  w1
[2025-11-17 21:35:38] INFO:     Op: matmul
[2025-11-17 21:35:38] INFO:     Shape: 8192x28672 (batch=32)
[2025-11-17 21:35:38] INFO:     Latency: 110877.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 466473.000000
[2025-11-17 21:35:38] INFO: 
  w3
[2025-11-17 21:35:38] INFO:     Op: matmul
[2025-11-17 21:35:38] INFO:     Shape: 8192x28672 (batch=32)
[2025-11-17 21:35:38] INFO:     Latency: 110877.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 577350.000000
[2025-11-17 21:35:38] INFO: 
  silu
[2025-11-17 21:35:38] INFO:     Op: silu
[2025-11-17 21:35:38] INFO:     Output Shape: torch.Size([32, 1, 28672])
[2025-11-17 21:35:38] INFO:     Latency: 7168.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 584518.000000
[2025-11-17 21:35:38] INFO: 
  mul
[2025-11-17 21:35:38] INFO:     Op: vector_mul
[2025-11-17 21:35:38] INFO:     Output Shape: torch.Size([32, 1, 28672])
[2025-11-17 21:35:38] INFO:     Latency: 1792.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 586310.000000
[2025-11-17 21:35:38] INFO: 
  w2
[2025-11-17 21:35:38] INFO:     Op: matmul
[2025-11-17 21:35:38] INFO:     Shape: 28672x8192 (batch=32)
[2025-11-17 21:35:38] INFO:     Latency: 168656.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 754966.000000
[2025-11-17 21:35:38] INFO: [34/36] ✓ Meta-Llama-3-70B batch=64 strategy=h2llm latency=1257202.000000s
[2025-11-17 21:35:38] INFO: 
================================================================================
[2025-11-17 21:35:38] INFO: Detailed Latency: Meta-Llama-3-70B batch=64 strategy=h2llm
[2025-11-17 21:35:38] INFO: ================================================================================
[2025-11-17 21:35:38] INFO: 
  attention_norm
[2025-11-17 21:35:38] INFO:     Op: rmsnorm
[2025-11-17 21:35:38] INFO:     Output Shape: torch.Size([64, 1, 8192])
[2025-11-17 21:35:38] INFO:     Latency: 5120.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 5120.000000
[2025-11-17 21:35:38] INFO: 
  wq
[2025-11-17 21:35:38] INFO:     Op: matmul
[2025-11-17 21:35:38] INFO:     Shape: 8192x8192 (batch=64)
[2025-11-17 21:35:38] INFO:     Latency: 74156.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 79276.000000
[2025-11-17 21:35:38] INFO: 
  wk
[2025-11-17 21:35:38] INFO:     Op: matmul
[2025-11-17 21:35:38] INFO:     Shape: 8192x8192 (batch=64)
[2025-11-17 21:35:38] INFO:     Latency: 74156.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 153432.000000
[2025-11-17 21:35:38] INFO: 
  wv
[2025-11-17 21:35:38] INFO:     Op: matmul
[2025-11-17 21:35:38] INFO:     Shape: 8192x8192 (batch=64)
[2025-11-17 21:35:38] INFO:     Latency: 74156.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 227588.000000
[2025-11-17 21:35:38] INFO: 
  qk_matmul
[2025-11-17 21:35:38] INFO:     Op: batched_matmul
[2025-11-17 21:35:38] INFO:     Output Shape: torch.Size([64, 64, 1, 2048])
[2025-11-17 21:35:38] INFO:     Latency: 100233.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 327821.000000
[2025-11-17 21:35:38] INFO: 
  softmax
[2025-11-17 21:35:38] INFO:     Op: softmax
[2025-11-17 21:35:38] INFO:     Output Shape: torch.Size([64, 64, 1, 2048])
[2025-11-17 21:35:38] INFO:     Latency: 98304.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 426125.000000
[2025-11-17 21:35:38] INFO: 
  score_v_matmul
[2025-11-17 21:35:38] INFO:     Op: batched_matmul
[2025-11-17 21:35:38] INFO:     Output Shape: torch.Size([64, 64, 1, 128])
[2025-11-17 21:35:38] INFO:     Latency: 100233.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 526358.000000
[2025-11-17 21:35:38] INFO: 
  wo
[2025-11-17 21:35:38] INFO:     Op: matmul
[2025-11-17 21:35:38] INFO:     Shape: 8192x8192 (batch=64)
[2025-11-17 21:35:38] INFO:     Latency: 74156.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 600514.000000
[2025-11-17 21:35:38] INFO: 
  ffn_norm
[2025-11-17 21:35:38] INFO:     Op: rmsnorm
[2025-11-17 21:35:38] INFO:     Output Shape: torch.Size([64, 1, 8192])
[2025-11-17 21:35:38] INFO:     Latency: 5120.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 605634.000000
[2025-11-17 21:35:38] INFO: 
  w1
[2025-11-17 21:35:38] INFO:     Op: matmul
[2025-11-17 21:35:38] INFO:     Shape: 8192x28672 (batch=64)
[2025-11-17 21:35:38] INFO:     Latency: 211216.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 816850.000000
[2025-11-17 21:35:38] INFO: 
  w3
[2025-11-17 21:35:38] INFO:     Op: matmul
[2025-11-17 21:35:38] INFO:     Shape: 8192x28672 (batch=64)
[2025-11-17 21:35:38] INFO:     Latency: 211216.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 1028066.000000
[2025-11-17 21:35:38] INFO: 
  silu
[2025-11-17 21:35:38] INFO:     Op: silu
[2025-11-17 21:35:38] INFO:     Output Shape: torch.Size([64, 1, 28672])
[2025-11-17 21:35:38] INFO:     Latency: 14336.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 1042402.000000
[2025-11-17 21:35:38] INFO: 
  mul
[2025-11-17 21:35:38] INFO:     Op: vector_mul
[2025-11-17 21:35:38] INFO:     Output Shape: torch.Size([64, 1, 28672])
[2025-11-17 21:35:38] INFO:     Latency: 3584.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 1045986.000000
[2025-11-17 21:35:38] INFO: 
  w2
[2025-11-17 21:35:38] INFO:     Op: matmul
[2025-11-17 21:35:38] INFO:     Shape: 28672x8192 (batch=64)
[2025-11-17 21:35:38] INFO:     Latency: 211216.000000
[2025-11-17 21:35:38] INFO:     Cumulative: 1257202.000000
[2025-11-17 21:36:22] INFO: [35/36] ✓ Meta-Llama-3-70B batch=64 strategy=recursive_grid_search latency=923613.000000s
[2025-11-17 21:36:22] INFO: 
================================================================================
[2025-11-17 21:36:22] INFO: Detailed Latency: Meta-Llama-3-70B batch=64 strategy=recursive_grid_search
[2025-11-17 21:36:22] INFO: ================================================================================
[2025-11-17 21:36:22] INFO: 
  fused_matrix_2195349239280
[2025-11-17 21:36:22] INFO:     Op: fusion_matrix
[2025-11-17 21:36:22] INFO:     Shape: 8192x24576 (batch=64)
[2025-11-17 21:36:22] INFO:     Latency: 153934.000000
[2025-11-17 21:36:22] INFO:     Cumulative: 153934.000000
[2025-11-17 21:36:22] INFO: 
  qk_matmul
[2025-11-17 21:36:22] INFO:     Op: batched_matmul
[2025-11-17 21:36:22] INFO:     Output Shape: torch.Size([64, 64, 1, 2048])
[2025-11-17 21:36:22] INFO:     Latency: 81786.000000
[2025-11-17 21:36:22] INFO:     Cumulative: 235720.000000
[2025-11-17 21:36:22] INFO: 
  score_v_matmul
[2025-11-17 21:36:22] INFO:     Op: batched_matmul
[2025-11-17 21:36:22] INFO:     Output Shape: torch.Size([64, 64, 1, 128])
[2025-11-17 21:36:22] INFO:     Latency: 100534.000000
[2025-11-17 21:36:22] INFO:     Cumulative: 336254.000000
[2025-11-17 21:36:22] INFO: 
  wo
[2025-11-17 21:36:22] INFO:     Op: matmul
[2025-11-17 21:36:22] INFO:     Shape: 8192x8192 (batch=64)
[2025-11-17 21:36:22] INFO:     Latency: 57682.000000
[2025-11-17 21:36:22] INFO:     Cumulative: 393936.000000
[2025-11-17 21:36:22] INFO: 
  fused_matrix_2195349241152
[2025-11-17 21:36:22] INFO:     Op: fusion_matrix
[2025-11-17 21:36:22] INFO:     Shape: 8192x57344 (batch=64)
[2025-11-17 21:36:22] INFO:     Latency: 348441.000000
[2025-11-17 21:36:22] INFO:     Cumulative: 742377.000000
[2025-11-17 21:36:22] INFO: 
  w2
[2025-11-17 21:36:22] INFO:     Op: matmul
[2025-11-17 21:36:22] INFO:     Shape: 28672x8192 (batch=64)
[2025-11-17 21:36:22] INFO:     Latency: 181236.000000
[2025-11-17 21:36:22] INFO:     Cumulative: 923613.000000
[2025-11-17 21:36:22] INFO: [36/36] ✓ Meta-Llama-3-70B batch=64 strategy=trivial latency=1505264.000000s
[2025-11-17 21:36:22] INFO: 
================================================================================
[2025-11-17 21:36:22] INFO: Detailed Latency: Meta-Llama-3-70B batch=64 strategy=trivial
[2025-11-17 21:36:22] INFO: ================================================================================
[2025-11-17 21:36:22] INFO: 
  attention_norm
[2025-11-17 21:36:22] INFO:     Op: rmsnorm
[2025-11-17 21:36:22] INFO:     Output Shape: torch.Size([64, 1, 8192])
[2025-11-17 21:36:22] INFO:     Latency: 5120.000000
[2025-11-17 21:36:22] INFO:     Cumulative: 5120.000000
[2025-11-17 21:36:22] INFO: 
  wq
[2025-11-17 21:36:22] INFO:     Op: matmul
[2025-11-17 21:36:22] INFO:     Shape: 8192x8192 (batch=64)
[2025-11-17 21:36:22] INFO:     Latency: 99379.000000
[2025-11-17 21:36:22] INFO:     Cumulative: 104499.000000
[2025-11-17 21:36:22] INFO: 
  wk
[2025-11-17 21:36:22] INFO:     Op: matmul
[2025-11-17 21:36:22] INFO:     Shape: 8192x8192 (batch=64)
[2025-11-17 21:36:22] INFO:     Latency: 99379.000000
[2025-11-17 21:36:22] INFO:     Cumulative: 203878.000000
[2025-11-17 21:36:22] INFO: 
  wv
[2025-11-17 21:36:22] INFO:     Op: matmul
[2025-11-17 21:36:22] INFO:     Shape: 8192x8192 (batch=64)
[2025-11-17 21:36:22] INFO:     Latency: 99379.000000
[2025-11-17 21:36:22] INFO:     Cumulative: 303257.000000
[2025-11-17 21:36:22] INFO: 
  qk_matmul
[2025-11-17 21:36:22] INFO:     Op: batched_matmul
[2025-11-17 21:36:22] INFO:     Output Shape: torch.Size([64, 64, 1, 2048])
[2025-11-17 21:36:22] INFO:     Latency: 100233.000000
[2025-11-17 21:36:22] INFO:     Cumulative: 403490.000000
[2025-11-17 21:36:22] INFO: 
  softmax
[2025-11-17 21:36:22] INFO:     Op: softmax
[2025-11-17 21:36:22] INFO:     Output Shape: torch.Size([64, 64, 1, 2048])
[2025-11-17 21:36:22] INFO:     Latency: 98304.000000
[2025-11-17 21:36:22] INFO:     Cumulative: 501794.000000
[2025-11-17 21:36:22] INFO: 
  score_v_matmul
[2025-11-17 21:36:22] INFO:     Op: batched_matmul
[2025-11-17 21:36:22] INFO:     Output Shape: torch.Size([64, 64, 1, 128])
[2025-11-17 21:36:22] INFO:     Latency: 100233.000000
[2025-11-17 21:36:22] INFO:     Cumulative: 602027.000000
[2025-11-17 21:36:22] INFO: 
  wo
[2025-11-17 21:36:22] INFO:     Op: matmul
[2025-11-17 21:36:22] INFO:     Shape: 8192x8192 (batch=64)
[2025-11-17 21:36:22] INFO:     Latency: 99379.000000
[2025-11-17 21:36:22] INFO:     Cumulative: 701406.000000
[2025-11-17 21:36:22] INFO: 
  ffn_norm
[2025-11-17 21:36:22] INFO:     Op: rmsnorm
[2025-11-17 21:36:22] INFO:     Output Shape: torch.Size([64, 1, 8192])
[2025-11-17 21:36:22] INFO:     Latency: 5120.000000
[2025-11-17 21:36:22] INFO:     Cumulative: 706526.000000
[2025-11-17 21:36:22] INFO: 
  w1
[2025-11-17 21:36:22] INFO:     Op: matmul
[2025-11-17 21:36:22] INFO:     Shape: 8192x28672 (batch=64)
[2025-11-17 21:36:22] INFO:     Latency: 221753.000000
[2025-11-17 21:36:22] INFO:     Cumulative: 928279.000000
[2025-11-17 21:36:22] INFO: 
  w3
[2025-11-17 21:36:22] INFO:     Op: matmul
[2025-11-17 21:36:22] INFO:     Shape: 8192x28672 (batch=64)
[2025-11-17 21:36:22] INFO:     Latency: 221753.000000
[2025-11-17 21:36:22] INFO:     Cumulative: 1150032.000000
[2025-11-17 21:36:22] INFO: 
  silu
[2025-11-17 21:36:22] INFO:     Op: silu
[2025-11-17 21:36:22] INFO:     Output Shape: torch.Size([64, 1, 28672])
[2025-11-17 21:36:22] INFO:     Latency: 14336.000000
[2025-11-17 21:36:22] INFO:     Cumulative: 1164368.000000
[2025-11-17 21:36:22] INFO: 
  mul
[2025-11-17 21:36:22] INFO:     Op: vector_mul
[2025-11-17 21:36:22] INFO:     Output Shape: torch.Size([64, 1, 28672])
[2025-11-17 21:36:22] INFO:     Latency: 3584.000000
[2025-11-17 21:36:22] INFO:     Cumulative: 1167952.000000
[2025-11-17 21:36:22] INFO: 
  w2
[2025-11-17 21:36:22] INFO:     Op: matmul
[2025-11-17 21:36:22] INFO:     Shape: 28672x8192 (batch=64)
[2025-11-17 21:36:22] INFO:     Latency: 337312.000000
[2025-11-17 21:36:22] INFO:     Cumulative: 1505264.000000
[2025-11-17 21:36:24] INFO: 
Finalizing results...
[2025-11-17 21:36:25] INFO: 
[2025-11-17 21:36:25] INFO: ================================================================================
[2025-11-17 21:36:25] INFO: Benchmark Completed
[2025-11-17 21:36:25] INFO: ================================================================================
[2025-11-17 21:36:25] INFO: Total tests: 36
[2025-11-17 21:36:25] INFO: Successful: 36
[2025-11-17 21:36:25] INFO: Failed: 0
[2025-11-17 21:36:25] INFO: Results: results\benchmark_results_20251117_205640.csv
[2025-11-17 21:36:25] INFO: Log: logs\benchmark_20251117_205640.log
